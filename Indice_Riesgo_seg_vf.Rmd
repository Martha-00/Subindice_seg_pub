---
title: "Subíndice de riesgo de seguridad pública"
author: "Martha Aguilar Jiménez"
date: "04/10/2024"
output:
  pdf_document: default
  word_document: default
---
Se instalan las librerías que se necesitarán para realizar análisis de componentes principales y análisis de factores.

```{r, warning=FALSE,error=FALSE,message=FALSE}
# Se limpia la memoria
rm(list = ls())
invisible(gc(reset = TRUE))
library(readr)             #Para leer archivos
library(corrplot)          #Gráficar las correlaciones
library(foreign)           # Leer archivos dbf
library(readxl)            # Leer archivos xlsx
library(stats)             # Cálculos estadísticos
library(psych)             # AF por componentes principales
library(ggplot2)           # Gráficos basados en grammar of graphics
library(GGally)            # Para realizar las gráficas ggpairs
library(kableExtra)        # Formato de tablas en R Markdown
library(dplyr)             # Manipulación de datos
library(FactoMineR)        #Principales métodos de componentes principales
library(factoextra)        #Proporciona las funciones para visualizar los resultados del acp

# Se cargan las librerías necesarias para realizar el mapa. 
library(sf)         # Para manejar datos espaciales
library(tidyr)      # Para transformación de datos
#(dplyr)      # Para manipulación de datos
#library(ggplot2)    # Para la visualización
```

Se leen los datos

```{r}
# Se establece el directorio de trabajo
setwd("C:/Users/User/Documents/ÍNDICE DE RIESGO/Bases de datos")
## Se cargan los datos de interés
datos<-read_excel("BASES DE DATOS.xls",sheet ="SEG_2023")
datos<-as.data.frame(datos)
datos$cve<-c(01,02,03,04,05,06,07,08,09,10,11,12,13,14,15,
             16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32)
rownames(datos)<- datos[,"cve"]
seg <- datos[, -c(1,11,20)]# Es la base con todas las variables de interés.
```

## 1.1 Análisis Exploratorio
Se observan las características del data frame *seg*.

```{r}
str(seg) #32 observaciones y 17 variables (falta el indicador 10)
class(seg)# Todas las variables son numéricas y son data frame.
```
Como se observa se tiene un data frame con 17 variables numéricas y con 32 observaciones.

Se continua con el análisis exploratorio de los datos.

```{r}
summary(seg)#Solo la variable país es de tipo carácter, las demás son numéricas
```
Se observa que las 17 variables analizadas tienen diferentes valores muy dispersos, además, se observa que en ningún caso hay valores faltantes.

1. La variable "*seg_1_2023: Tasa de homicidios por cada cien mil habitantes*" en 2021 tuvo un valor mínimo de 2.338, un valor máximo de 108.417 y una media de 30.265, es decir, en promedio hubieron 30 homicidios por cada cien mil hab.
2. La variable "*seg_2_2023: Incidencia delictiva por cada cien mil habitantes*" durante 2021 registró un valor mínimo de 16386, un valor máximo de 45501 y en promedio hubieron 27333 delitos por cada cien mil hab.
3. La variable "*seg_3_2023: Prevalencia delictiva por cada cien mil habitantes*" durante 2021 registró un valor mínimo de 14082, un valor máximo de 38253 y en promedio hubieron 21650 víctimas por cada cien mil hab.
4. La variable "*seg_4_2023: Cifra negra de delitos en la entidad federativa*" durante 2021 registró un valor mínimo de 89.54%, un valor máximo de 96.70% y en promedio se registró una cifra negra de 93.09%, es decir, es el porcentaje de delitos no captados por la autoridad debido a que la víctima no denunció el delito o lo denunció pero no se inició una averiguación previa.
5. La variable "*seg_5_2023: Prevalencia delictiva por cada diez mil unidades económicas*" durante 2021 registró un valor mínimo de 1288, un valor máximo de 3687 y en promedio hubieron 2534 unidades económicas que fueron víctimas de algún delito por cada diez mil unidades económicas.
6. La variable "*seg_6_2023: Cifra negra de delitos en unidades económicas*" durante 2021 registró un valor mínimo de 75.79%, un valor máximo de 97.27% y en promedio se registró una cifra negra de 91.80%, es decir, es el porcentaje de delitos no captados por la autoridad debido a que la unidad económica víctima no denunció el delito o lo denunció pero no se inició una averiguación previa a nivel nacional.
7. La variable "*seg_7_2023: Índice de Riesgo por Crimen Organizado*" durante el segundo trimestre registró un valor mínimo de 3.30, un valor máximo de 85.50 y en promedio el índice se registró 41.60.
8. La variable "*seg_8_2023: Promedio de costos del delito contra personas, por entidad federativa*" durante 2021 registró un valor mínimo de 4437, un valor máximo de 12053 y en promedio los costos del delito contra personas fue de 7119.
9. La variable "*seg_9_2023: Promedio de costos del delito en unidades económicas, por entidad federativa*" durante 2021 registró un valor mínimo de 22542, un valor máximo de 95631 y en promedio los costos del delito en unidades económicas fue de 49302.
10. La variable "*seg_10_2023: Porcentaje del gasto público destinado a seguridad pública en la entidad federativa*" no se calculo.
11. La variable "*seg_11_2023: Porcentaje de personas que consideran insegura su entidad federativa*" durante 2022 registró un valor mínimo de 30.59%, un valor máximo de 90.85% y en promedio el porcentaje fue de 70.17%.
12. La variable "*seg_12_2023: Porcentaje de personas que cambió algún hábito por temor a sufrir algún delito*" durante 2022 registró un valor mínimo de 13.75%, un valor máximo de 46.58% y en promedio el porcentaje fue de 29.26%.
13. La variable "*seg_13_2023: Tasa de policías por cada cien mil habitantes*" durante 2021 registró un valor mínimo de 128.1, un valor máximo de 976.5 en promedio hubieron 231.9 policías estatales y municipales por cada cien mil hab.
14. La variable "*seg_14_2023: Desconfianza en la policía estatal y municipal*" durante 2022 registró un valor mínimo de 27.57, un valor máximo de 57.08 y en promedio hubo una desconfianza en la policía estatal y municipal de 42.02.
15. La variable "*seg_15_2023: Desconfianza en jueces*" durante 2022 registró un valor mínimo de 25.71, un valor máximo de 59.63 y en promedio hubo una desconfianza en los jueces de 36.78.
16. La variable "*seg_16_2023: Desconfianza en ministerios públicos*" durante 2022 registró un valor mínimo de 20.23, un valor máximo de 63.95 y en promedio hubo una desconfianza en ministerios públicos de 38.08.
17. La variable "*seg_17_2023: Variación porcentual del índice de conflictividad en la entidad federativa*" durante el trimestre de 2023 registró un valor mínimo de 0.5074, un valor máximo de 5.2419 y en promedio la variación fue de 2.5965.
18. La variable "*seg_18_2023: Índice de Impunidad estatal en materia penal en entidades federativas*" durante el 2021 registró un valor mínimo de 69.50, un valor máximo de 98.40 y en promedio el índice de impunidad estatal fue de 90.43.


## 1.2 Visualización de los datos

Podemos, además apoyarnos de una gráfica, en donde nos mostrará 3 elementos importantes:
1. Los gráficos de dispersión en los cuales podremos ver si existen relaciones lineales entre las variables.
2. Mostrará también las densidades de las variables.
3. Y finalmente, tendremos las correlaciones de las variables.
```{r}
# Función para gráficos de dispersión
lowerFn <- function(data, mapping, method = "lm", ...) {
p <- ggplot(data = data, mapping = mapping) +
     geom_point(colour = "blue") +
     geom_smooth(method = method, color = "red", ...)
p 
}

# Matriz de gráficos de dispersión parte 1
ggpairs(seg[, 1:4], lower = list(continuous = wrap(lowerFn, method = "lm")),
        diag = list(continuous = wrap("barDiag", colour = "blue")), 
        upper = list(continuous = wrap("cor", size = 4)), 
        title = "Matriz de gráficos de dispersión - parte 1")
```

En la matriz de gráficos anterior se aprecia que, la dispersión es variada entre variables, sin embargo, la que completamente sigue la tendencia lineal es la del par de variables seg_2_2023 y seg_3_2023 la cual tiene la mayor correlación lineal de 0.936, en contraste con el par de variables seg_1_2023 y seg_3_2023, cuyo coeficiente de correlación es el menor con un valor de 0.042.

```{r}
# Matriz de gráficos de dispersión parte 2
ggpairs(seg[, 5:8], lower = list(continuous = wrap(lowerFn, method = "lm")),
        diag = list(continuous = wrap("barDiag", colour = "blue")), 
        upper = list(continuous = wrap("cor", size = 4)), 
        title = "Matriz de gráficos de dispersión - parte 2")
```
De acuerdo con la matriz de gráficos previa, se observa que la mayoría de las gráficas muestran gran dispersión entre variables, con estas variables, la que mayor correlación lineal tiene es de 0.386, que corresponde al par de variables seg_7_2023 y seg_8_2023 mientras que, la de menor correlación corresponde al par de variables seg_5_2023 y seg_8_2023 con un valor de 0.029. Con respecto a la densidad la variable seg_7_2023 es la que menos se parece a la normal.


```{r}
# Matriz de gráficos de dispersión parte 3
ggpairs(seg[, 9:12], lower = list(continuous = wrap(lowerFn, method = "lm")),
        diag = list(continuous = wrap("barDiag", colour = "blue")), 
        upper = list(continuous = wrap("cor", size = 4)), 
        title = "Matriz de gráficos de dispersión - parte 3")
```
De acuerdo con la matriz de gráficos previa, la que completamente sigue la tendencia lineal es la del par de variables seg_11_2023 y seg_12_2023, con una correlación lineal de 0.789, mientras que, la de menor correlación corresponde al par de variables seg_12_2023 y seg_13_2023  con un valor de 0.052. Con respecto a las densidades, se aprecian que siguen una tendencia normal con diferentes características.


```{r}
# Matriz de gráficos de dispersión parte 4
ggpairs(seg[, 13:16], lower = list(continuous = wrap(lowerFn, method = "lm")),
        diag = list(continuous = wrap("barDiag", colour = "blue")), 
        upper = list(continuous = wrap("cor", size = 4)), 
        title = "Matriz de gráficos de dispersión - parte 4")
```
De acuerdo con la matriz de gráficos previa, se observa que varios pares siguen una tendencia lineal, el par de variables con mayor tendencia lineal es seg_15_2023 y seg_16_2023, con una correlación lineal de 0.89, mientras que, la de menor correlación corresponde al par de variables seg_14_2023 y seg_17_2023 con un valor de 0.019. Con respecto a las densidades de cada variable, se observa un parecido con la distribución gaussina.

```{r, error=TRUE, echo=TRUE}
# Matriz de gráficos de dispersión parte 5
ggpairs(seg[,17:18], lower = list(continuous = wrap(lowerFn, method = "lm")),
        diag = list(continuous = wrap("barDiag", colour = "blue")), 
        upper = list(continuous = wrap("cor", size = 4)), 
        title = "Matriz de gráficos de dispersión - parte 5")
#La siguiente salida marcó error,se soluciona en cuanto se tenga información de la variable faltante.
```
#Este error se soluciona en cuanto se tenga información de la variable "seg_10_2023".

```{r}
ggpairs(seg,diag=list(continuous="densityDiag"),upper = list(continuous = wrap("cor", size = 2)))
```

Con respecto a las densidades se espera que tengan una distribución gaussiana, vemos en la gráfica que 15 variables tienen un parecido a una campana de gauss, sin embargo, las variables *seg_7_2023* y *seg_13_2023* no tienen esa aproximación, sin embargo, se van a seguir considerando. Con respecto a las relaciones lineales entre variables, en la gráfica se puede apreciar que si hay variables relacionadas y finalmente, con respecto a la correlación entre variables, también se observan algunas correlaciones interesantes.

## 1.3 Estandarización de los datos

Antes de continuar es importante mencionar que cuando se estandarizan los datos la matriz de covarianzas y de correlaciones son prácticamente las mismas. La estandarización evita el problema de tener una variable con gran varianza.

Se realiza la gráfica de boxplot con los datos estandarizados.

```{r}
#Se estandarizan los datos los datos
est_seg<- scale(seg, center = TRUE, scale = TRUE)

#Se realizan los diagramas de cajas
boxplot(seg, col ="#FFF89A", main= "Datos sin estandarizar")
``` 

De acuerdo con el gráfico de caja y bigotes, la variable seg_9_2023 tiene valores muy altos lo que implica que las demás variables casi no se visualicen, además, se observan que algunas variables registran algunos valores atípicos aunque son pocos.
Entonces, por lo anterior las escalas estén muy desfasadas, por lo tanto se recomienda estandarizar los datos para que sean comparables.

Se realiza el gráfico estandarizando.

```{r}
boxplot(est_seg, col ="#FEBE8C", main="Datos estandarizados")
```

Como se aprecia en el gráfico anterior, ya las variables resultan más parejas, pues ya todas oscilan alrededor del cero, aunque se observa que hay 9 variables que reportan uno o dos valores atípicos.

# 2. Análisis de componentes principales.

El Análisis de Componentes Principales (ACP) es una técnica estadística de síntesis de la información, o reducción de la dimensión (número de variables). Es decir, ante un banco de datos con muchas variables, el objetivo será reducirlas a un menor número perdiendo la menor cantidad de información posible.  Los nuevos componentes principales o factores serán una combinación lineal de las variables originales, y además serán independientes entre sí.

Un aspecto clave en ACP es la interpretación de los factores, ya que ésta no viene dada a priori, sino que será deducida tras observar la relación de los factores con las variables iniciales (habrá, pues, que estudiar tanto el signo como la magnitud de las correlaciones). Esto no siempre es fácil, y será de vital importancia el conocimiento que el experto tenga sobre la materia de investigación. 

En el caso del aprendizaje no supervisado, la variable respuesta no se tiene en cuenta ya que el objetivo no es predecir, sino extraer información empleando los predictores, por ejemplo, para identificar sub-grupos. El principal problema al que se enfrentan este tipo de métodos es la dificultad para validar los resultados, dado que no se dispone de una variable respuesta que permita contrastarlos.

El método de PCA permite, por lo tanto, "condensar" la información aportada por múltiples variables en solo unas pocas componentes. Esto lo convierte en un método muy útil de aplicar previa utilización de otras técnicas estadísticas tales como regresión, clustering, etc. Aun así no hay que olvidar que sigue siendo necesario disponer del valor de las variables originales para calcular las componentes.

## 2.1 Técnica de Análisis de Componentes Principales.

Se aplicará un análisis de componentes principales para reducir la dimensionalidad de los datos, esto para una mejor interpretación de qué es lo que explica al fenómeno de interés, es decir, se busca una relación entre las variables estudiadas de seguridad pública.

Para que un PCA tenga sentido, es necesario que exista una alta correlación entre los predictores, por lo que se explorará a continuación.

```{r}
#seleccionamos los datos numéricos
cor_seg <-cor(est_seg)
#Gráfica de correlaciones entre variables
corrplot(cor_seg, tl.cex = 1, tl.col = "black", mar = c(0, 0, 0, 0))
```

Derivado de la gráfica previa, se observa que en efecto sí existen correlaciones altas entre algunas variables. La correlación mide la fuerza y dirección de la interacción entre dos variables, suponiendo relación lineal.
Por ejemplo, la variable *seg_1_2023* tiene correlación positiva fuerte con la variable seg_7_2023, le sigue la *seg_11_2023*, *seg_12_2023* y *seg_14_2023*.
En general algunas variables tiene correlaciones fuertes positivas, pero también se observan ciertas correlaciones negativas entre algunas variables.  
Finalmente, se calcula el determinante, se busca que el determinante se cercano a cero, pues implicaría un alta multicolinalidad entre variables.

```{r}
det_seg<-det(cor_seg) #Se calcula el determinante, como es cercano a cero, quiere decir que hay correlación.
det_seg #2.013597e-07
```

Como el determinante es 2.013597e-07, cercano a cero, quiere decir que hay correlación.
Derivado de lo anterior, el análisis de componentes principales es factible.

## 2.3 Varianza explicada.

```{r}
### PCA

seg.pc <- princomp(est_seg)
summary(seg.pc)

```
 
Uno de los criterios para escoger el número adecuado de componentes principales, es cuidar que la varianza acumulada que expliquen, sea de por lo menos 70%, y de acuerdo con la salida del modelo, se necesitan cinco componentes, en el ejemplo se acumula el 75.8% de la varianza del modelo. Aunque también se podría proponer 4 componentes, ya que se acumula el 69% de la varianza, que es muy cercano a 70%. 

Otros criterios son el del codo, que a partir del gráfico de sedimentación, se tiene que ubicar el cambio de trayectoria más abrupta, y en ese punto, será el número de componentes adecuado, otro es el de Kaiser, que indica conservar aquellas componentes cuyos valores propios superen a la unidad.

```{r}
scree(est_seg,main = "Gráfico de sedimentación", pc=TRUE, fa=FALSE)
```

Según el gráfico anterior, el criterio del codo sugiere dos componentes, mientras que la regla de Kaiser sugiere cinco. En este caso, los criterios probados indican un número diferente de componentes, sin embargo, se va a priorizar la eficiencia del modelo, es decir, se van a elegir cinco componentes en favor de explicar lo más que se pueda de la varianza acumulada, esto pese a la idea de la técnica, que es reducir la dimensión.

## 2.4 Extracción de cargas por componentes principales

```{r}
# Elegimos 5 comp
#Se cachan los vectores propios
pc1 <- seg.pc$loadings[,1]
pc2 <- seg.pc$loadings[,2]
pc3 <- seg.pc$loadings[,3]
pc4 <- seg.pc$loadings[,4]
pc5 <- seg.pc$loadings[,5]

#Se realizan las gráficas
plot(pc1, ylim = c(-1, 1.5), type = "l", lwd=2)
lines(pc2, col="blue", lwd=2)
lines(pc3, col="green", lwd=2)
lines(pc4, col="purple", lwd=2)
lines(pc5, col="orange", lwd=2)
abline(h=0, lty=2, col="red")
legend(x="topright", legend = c("PC1", "PC2","PC3", "PC4","PC5"),
       fill = c("black", "blue", "green", "purple","orange"))
```

En la gráfica se muestra el comportamiento de los primeros 5 componentes principales respecto y la relación con las variables originales.

Según las cargas de los componentes, por lo menos visualmente hay algunas que destacan en alguna variable, por ejemplo las cargas del cuarto componente (morado), tiene un pico muy marcado hacia abajo en la variable seg_8_2023, que corresponde al *promedio de costos del delito contra personas, por entidad federativa*,además de que tiene un pico pronunciado marcado hacia arriba en la variable seg_4_2023  correspondiente a la *Cifra negra de delitos en la entidad federativa*, por lo que se podría pensar que las dos variables mencionadas son las que define al componente cuatro.

1. En el primer componente hay 7 variables que tienen pesos similares y son los más altos, rondando en valore de 0.3.
2. Para el segundo componente hay 2 variables con cargas positivas elevadas las cuales son seg_7_2023, seg_1_2023 con cargas de 0.508 y 0.502 respectivamente, además, en tercer lugar quedó la variable seg_13_2023 con carga de -0.343 y ésta última hace contraste con los componentes antes mencionados.
3. En el componente 3, se observa que la variable con mayor carga la tiene la variable seg_7_2023 con un valor de 0.584,y luego, la carga de la variable seg_6_2023 con el siguiente valor mayor es 0.430.
4. En el componente 4, se puede considerar como un componente de contraste, pues se observa que la variable con mayor carga (negativa) la tiene la variable seg_8_2023 con un valor de -0.55, y la variable seg_4_2023 hace contraste con un valor de 0.536, le sigue la variable seg_6_2023 con un peso de 0.467.
5. Finalmente, en el componente 5 la variable que mayor carga es seg_5_2023 con un peso de 0.671 y las demás variables, tienen pesos muy pequeños en este componente.

Para aterrizar lo anterior, se dibuja el diagrama donde se indica qué variables son las que explican a cada uno de los componentes.

```{r}
fa.diagram(seg.pc$loadings[,1:5], main = "Compontes principales")
```

Según el diagrama anterior, el componente uno está definido por las variables:seg_14_2023, seg_15_2023, seg_16_2023, seg_2_2023, seg_3_2023, seg_11_2023 y seg_12_2023 todas en sentido positivo y con una carga muy similar, este componente puede estar explicando el desempeño institucional,percepción de inseguridad y cambio de rutinas y Nivel de criminalidad (incidencia delictiva). El componente dos se define por seg_7_2023, seg_1_2023 y seg_13_2023, éste último con carga negativa, que se refiere a temas con el crimen organizado, Nivel de criminalidad (Tasa de homicidios) y la tasa de policías. El componente tres se define por la variable seg_17_2023 con carga positiva, relacionados con el desempeño institucional en particular con la variación porcentual del índice de conflictividad. El componente cuatro se define por las variables seg_8_2023 (carga negativa), seg_4_2023 y seg_6_2023 referente al costo del delito y nivel de criminalidad. Y finalmente, el componente cinco esta definido por las variables seg_5_2023 referente al nivel de criminalidad en particular con el indicador: prevalencia delictiva por cada diez mil unidades económicas.

Ahora, se pueden graficar los scores (a nivel dato) por pares de los componentes principales.
Por temas de interpretación, solo se graficarán los scores de los dos primeros componentes principales.

## 2.5 Gráficas de los scores

Ahora se graficaran los scores de los pares (comp1 vs comp2), (comp1 vs comp3) y (comp2 vs comp3).

Biplot componente 1 vs componente 2

Primero se extraen los puntajes de las observaciones (scores).
```{r}
#Se extraen los scores
score1<-seg.pc$scores[,1]
score2<-seg.pc$scores[,2]
score3<-seg.pc$scores[,3]
score4<-seg.pc$scores[,4]
score5<-seg.pc$scores[,5]
```

Se realiza el gráfico score 1 vs score 2

```{r}
#Se realiza el gráfico

plot(score1,score2,main="Biplot score 1 vs score 2",xlim=c(-5,7),ylim=c(-5,4.5),
 xlab="score 1",ylab="score 2",type="n",lwd=2)
text(score1,score2,
 labels=abbreviate(row.names(seg),minlength=8),cex=0.6,lwd=2, col = "blue")
abline(v = 0, h = 0, lty =2, col ="red")

```

Por ejemplo, la entidad 9 (CDMX), para el componente 1, registra un valor score muy alto (mayores niveles de incidencia, mayor percepción de inseguridad y finalmente mucha desconfianza en las autoridades), sin embargo, en el componente 2 relacionado con una baja tasa de homicidios, bajos niveles en el indice por crimen organizado y una alta tasa de policías, podría concluirse que a pesar de que hay altos niveles delictivos, y la desconfianza en la pob es alta, uno de los delitos más daniños en la sociedad tiene registros muy bajos. Y en el otro extremo está la entidad 31 (Yucatán) que registra menores tasas de incidencia se podría catalogar como la entidad con menos problemas de inseguridad.

Se realiza el gráfico score 1 vs score 3 

```{r}
#Se realiza el gráfico

plot(score1,score3,main="Biplot score 1 vs score 3",xlim=c(-5,7),ylim=c(-3.5,3.5),
 xlab="score 1",ylab="score 3",type="n",lwd=2)
text(score1,score3,
 labels=abbreviate(row.names(seg),minlength=8),cex=0.6,lwd=2, col = "blue")
abline(v = 0, h = 0, lty =2, col ="red")
```
Para esta combinación de scores, se tiene que la entidad 9 (CDMX) para el componente 1, registra un valor score muy alto (mayores niveles de incidencia, mayor percepción de inseguridad y finalmente mucha desconfianza en las autoridades),además con respecto al valor del score 3 es un valor medianamente alto, el cual está relacionado con la variación porcentual del índice de conflictividad en la entidad federativa.
En el caso de la entidad 15 (EDOMEX) para el componente 1, registra un valor score muy alto (mayores niveles de incidencia, mayor percepción de inseguridad y finalmente mucha desconfianza en las autoridades), sin  embargo, el score 3 es un valor muy bajo, el cual se puede decir que la variación del índice de conflictividad no cambió en esa entidad.
En el otro extremo está la entidad 31 (Yucatán) que registra menores tasas de incidencia se podría catalogar sin embargo, con lo que respecta al score 3 está en el promedio, es decir, si hubo variación en su índice.
Finalmente, la endidad 19 (Nuevo León) para el componente 1, registra un valor score ni muy baos ni muy altos, sin  embargo, el score 3 es un valor muy bajo, el cual se puede decir que la variación del índice de conflictividad no cambió en esa entidad.



Se realiza el gráfico score 2 vs score 3

```{r}
#Se realiza el gráfico

plot(score2,score3,main="Biplot score 2 vs score 3",xlim=c(-5,4.5),ylim=c(-3.5,3.5),
 xlab="score 2",ylab="score 3",type="n",lwd=2)
text(score2,score3,
 labels=abbreviate(row.names(seg),minlength=8),cex=0.6,lwd=2, col = "blue")
abline(v = 0, h = 0, lty =2, col ="red")
```

Con respecto a las interpretaciones del score 2 vs el score 3 se tiene lo siguiente:
En el primer cuadrante se encuentra la entidad 9 (CDMX) con los menores puntajes del score 2 y que tienen que ver con la tasa de homicidios baja, el Índice de Riesgo por Crimen Organizado bajo y una tasa de policías muy alta y por otro lado, el score 3 que representa la cifra negra personal y de empresa medianamente alta y con un costo del delito alto.
Para la entidad 19 (Nuevo León) el score 2 se puede interpretar como un valor promedio, sin embargo el score 3, registra un valor muy bajo y esto porque la cifra negra de delitos en unidades económicas registra el valor más bajo entre todas las entidades federativas.
Finalmente, la entidad 32 (Zacatecas) registra valores muy altos en el score 2 y también registra valores muy altos en el score 3 que tienen que ver con un alto porcentaje de delitos no denunciados, así como un costo del delito muy alto, es decir, es una entidad con altos niveles de violencia y alto porcentaje de no denuncia por parte de los ciudadanos y también les genera al costos por la inseguridad.

## 2.6 Cálculo del subíndice de seguridad

Ahora, se obtienen los scores para crear un data frame para hacer la escala del 0-100.

Opción 1. Se obtiene el promedio de los 5 scores, y sobre ese promedio se hace el escalamiento de 0-100. A continuación, se detalla el cálculo.

**NOTA 1:** El cálculo del subíndice de seguridad ***ind_seg*** se realizó de la siguiente manera:

a. A partir de los scores elegidos se obtiene la media de los mismos por entidad federativa y se nombra *media_seg*.

b. Se obtiene el valor mínimo de la nueva variable calculada *media_seg* y a ese valor se le asigna el nombre de *val_min*.

c. Se obtiene la diferencia de la media *(media_seg)* y el valor mínimo de la media *(val_min)* y se nombra *dif*.

d. Se obtiene el valor máximo de la diferencia *dif* y a ese valor se le asigna el nombre de *val_max*.

e. Se calcula el subíndice de seguridad, que es dividir la diferencia *dif* entre el valor máximo de la diferencia *val_max*, multiplicado por 100. De esta manera, se obtiene el valor del subíndice para cada una de las entidades federativas. 

```{r}
bd_score<-cbind(score1,score2,score3,score4,score5)
#Se convierte en data frame 
bd_score<- as.data.frame(bd_score)
bd_score$media_seg<- apply(bd_score,1,mean)
val_min<- min(bd_score$media_seg)# -1.57714
bd_score$dif<-(bd_score$media_seg-val_min)
val_max<- max(bd_score$dif)#3.192246
bd_score$ind_seg<-(bd_score$dif/val_max)*100
#bd_score #Se muestra el data frame construído, la última columna "ind_seg" es el índice de seguridad.
#Finalmente se agrega la variable entidad a la base final
bd_score$ID<-c("01","02","03","04","05","06","07","08","09",
               "10","11","12","13","14","15","16","17","18",
               "19","20","21","22","23","24","25","26","27",
               "28","29","30","31","32")
```

Opción 2. Se realiza el escalamiento por score y sobre eso se realiza el promedio. ¡Ojo en este no se obtendrá un escalamiento del 0-100 por el promedio.

**NOTA 2:** La segunda opción de cálculo del subíndice de seguridad ***ind_seg_f*** se realizó de la siguiente manera:

a. Se obtiene el valor mínimo de cada uno de los scores, de esta manera, se obtendrán *n* valores mínimos, donde *n* es el número de scores elegidos, en este ejemplo son 5 scores. A cada valor mínimo se le asigna un nombre, $val\_min_i$ y $i = 1,2,3...n$

b. Se obtiene la diferencia del $score_i$ y el valor mínimo de cada score $val\_min_i$ y se nombra $dif_i$

c. Se obtiene el valor máximo para cada una de las diferencias $dif_i$ y a ese valor se le asigna el nombre de $val\_max_i$.

d. Se calcula el subíndice de seguridad $ind\_seg_i$ que es dividir la diferencia $dif_i$ entre el valor máximo de la diferencia $val\_max_i$, multiplicado por 100. De esta manera, se obtiene el valor del $ind\_seg_i$ para cada una de las entidades federativas.

e. Finalmente, se promedian los $ind\_seg_i$ y se obtiene el subíndice de seguridad *ind_seg_f*

```{r}
bd_score1<-cbind(score1,score2,score3,score4,score5)
#Se onvierte en data frame 
bd_score1<- as.data.frame(bd_score1)
val_min1<- min(bd_score1$score1)
val_min2<- min(bd_score1$score2)
val_min3<- min(bd_score1$score3)
val_min4<- min(bd_score1$score4)
val_min5<- min(bd_score1$score5)
bd_score1$dif1<-(bd_score1$score1-val_min1)
bd_score1$dif2<-(bd_score1$score2-val_min2)
bd_score1$dif3<-(bd_score1$score3-val_min3)
bd_score1$dif4<-(bd_score1$score4-val_min4)
bd_score1$dif5<-(bd_score1$score5-val_min5)
val_max1<- max(bd_score1$dif1)
val_max2<- max(bd_score1$dif2)
val_max3<- max(bd_score1$dif3)
val_max4<- max(bd_score1$dif4)
val_max5<- max(bd_score1$dif5)
bd_score1$ind_seg1<-(bd_score1$dif1/val_max1)*100
bd_score1$ind_seg2<-(bd_score1$dif2/val_max2)*100
bd_score1$ind_seg3<-(bd_score1$dif3/val_max3)*100
bd_score1$ind_seg4<-(bd_score1$dif4/val_max4)*100
bd_score1$ind_seg5<-(bd_score1$dif5/val_max5)*100
bd_score1$ind_seg_f<-apply(bd_score1[,c(11:15)],1,mean)
#Finalmente se agrega la variable entidad a la base final
bd_score1$ID<-c("01","02","03","04","05","06","07","08","09",
                "10","11","12","13","14","15","16","17","18",
                "19","20","21","22","23","24","25","26","27",
                "28","29","30","31","32")
```


# 3. ANÁLISIS DE FACTORES

## Análisis factorial confirmatorio

Es una técnica basada en el análisis de estructuras de covarianzas que tiene como objetivo determinar si un modelo de medida especificado por el investigador, basándose en hipótesis teóricas o en un AFE previo, es consistente con la realidad. Para llegar a obtener una conclusión al respecto es preciso abordar una serie de fases comunes al conjunto de los procedimientos que operan con ecuaciones estructurales.  
\newline
**Especificación del modelo**

Se establece un modelo tomando en cuenta lo siguiente:  

-	Número de factores latentes comunes
-	Número de variables observables
-	Relación entre factores comunes
-	Relación entre variables observables y factores comunes
-	Relación entre factores únicos y variables observables
-	Relación entre factores únicos.

Para el establecimiento formal de la estructura del modelo, el investigador se basará en el AFE y/o en un sólido sustento teórico. A partir de esta base, deberá reflejar mediante ecuaciones la estructura relacional del modelo de medida, es decir, deberá especificar los parámetros del modelo que determinarán las relaciones existentes entre las variables observables y las latentes.  
\newline
**Identificación del modelo**  

A la hora de determinar si un modelo está o no identificado, caben 3 soluciones posibles:  

1. Que el modelo esté exactamente identificado, en cuyo caso se podrá estimar cada parámetro estructural a partir de una única combinación de los elementos de la matriz de covarianzas.
2. Que el modelo esté sobre identificado, es decir, al menos un parámetro podrá obtenerse a partir de dos o más ecuaciones diferentes.
3. Que el modelo esté infra identificado, en cuyo caso no será posible establecer ecuaciones de covarianza para alguno de los parámetros, por lo que no todos podrán ser estimados.  

La forma más efectiva de comprobar si un modelo está identificado es demostrando algebraicamente que es posible igualar cada parámetro estructural a una combinación de los elementos de la matriz de covarianza; como en la práctica esta tarea es tediosa y compleja, se han propuesto algunas reglas o condiciones que permiten determinar más fácilmente el estatus de la identificación del modelo. En este sentido, la regla *t*, según la cual el número de parámetros a estimar ha de ser igual o inferior al número de momentos no redundantes de la matriz de covarianzas:  
$$t\leq p(p+1)/2$$  

Donde *t* es el número de parámetros libres y *p* es el número de variables observables.  
\newline
**Estimación de los parámetros del modelo**  

Este proceso tiene como objetivo encontrar los valores que generen una matriz de covarianzas estimada tan próxima como sea posible a la matriz de covarianzas muestrales, que, supuestamente, será un estimador consistente de la matriz de covarianzas poblacional.  

El más común es el Método de Máxima Verosimilitud, que proporciona estimaciones consistentes, eficientes y no sesgadas cuando se cumple el supuesto de normalidad multivariante. Otros métodos son el de mínimos cuadrados generalizados o el de distribución asintótica libre.  
\newline
**Número de factores a seleccionar**  

Cuando se tienen algunas expectativas sobre la manera en que se agruparán las variables, se puede utilizar un criterio a priori, es decir, solicitar la extracción de un número de factores determinado.

Otro criterio, probablemente el más utilizado, es la regla de Gutman-Kaiser (o regla K1), esto es, conservar aquellos factores con valores propios mayores que 1. En este caso, resultaría útil consultar el gráfico de sedimentación (scree plot) y, además de aquellos factores que se encuentren antes del punto de inflexión del gráfico, retener algún factor más, siempre que su varianza se pueda considerar relativamente alta o próxima al límite establecido. 

Otras pautas para considerar son el análisis paralelo, que es muy similar al gráfico de sedimentación, y el comparativo de la matriz reproducida a partir del número de factores seleccionados, con la matriz original, a lo que se le denomina matriz de residuales. El porcentaje de la varianza total explicada es otro criterio que se puede utilizar para decidir el número de factores que serán retenidos, como umbral para la extracción de los factores se suele establecer un mínimo de 70%.  
\newline
**Generación de los scores**  

Habiendo determinado el número de factores seleccionados, se calculan las matrices de puntuaciones factoriales F, esto para conocer qué sujetos son los más raros o extremos, identificar dónde se ubican ciertos grupos o subcolectivos de la muestra, o simplemente para determinar qué factor sobresale uno del otro.

Los métodos de estimación más utilizados son por Regresión y Barlett.

* Método de Regresión: Estima F por el método de los mínimos cuadrados: $\widehat{F}=(A'A)^{-1} A'X$
* Método de Barlett: Utiliza el método de los mínimos cuadrados generalizados estimando las puntuaciones factoriales mediante:    $\widehat{F}=(A'\psi^{-1}A)^{-1}A'\psi^{-1}X$  

**Interpretación de resultados**  

La matriz de cargas factoriales (A) tiene un papel fundamental en la interpretación. Por otra parte, las cargas factoriales al cuadrado $(a^{2}_{il})$ indican si los factores son ortogonales, qué porcentaje de la variable original $(X_i)$ es explicado por el factor $F_l$.

A efectos prácticos, en la interpretación de los factores se sugiere:

* Identificar las variables cuyas correlaciones con el factor son las más elevadas en valor absoluto.
* Intentar dar un nombre a los factores. El nombre se asigna de acuerdo con la estructura de las correlaciones.
* Una ayuda en la interpretación de los factores puede ser la representación gráfica de los resultados obtenidos, sobre los ejes factoriales se proyectan las variables originales.
  * Las variables al final de un eje son aquellas que tienen correlaciones altas solo en ese factor y en consecuencia lo describen.
  * Las variables que están cerca del origen tienen correlaciones reducidas en ambos factores.
  * Las variables que no están cerca de ninguno de los ejes se relacionan con ambos factores.
* Eliminar las cargas factoriales bajas y de este modo suprimir información redundante. El investigador decide a partir de qué valor deben eliminarse las cargas factoriales, generalmente, se toma como significativas las cargas superiores a 0.5 en valor absoluto.

## 3.1 Análisis de factibilidad  

Antes de dar inicio con la implementación del modelo de factores como tal, es importante analizar la factibilidad de los datos, para ello se analiza la matriz de correlaciones, el cálculo del determinante de la matriz anterior y se generan las pruebas de Kaiser Meyer Olkin (KMO) y de esfericidad de Bartlet.

### 3.1.1 Prueba de Kaiser Meyer Olkin  

La prueba de Kaiser-Meyer-Olkin (KMO), es una medida de la idoneidad de los datos para el análisis factorial. Mide la adecuación del muestreo para cada variable en el modelo y para el modelo completo. La estadística es una medida de la proporción de varianza entre variables que podrían ser varianza común.

La prueba de KMO devuelve valores entre 0 y 1. Una regla general para interpretar la estadística es la siguiente: 

- Los valores de KMO entre 0.8 y 1 indican que el muestreo es perfecto para realizar un análisis de factores.
- Los valores entre 0.6 y 0.7 señalan que es viable realizar un análisis de factores.
- Los valores de KMO entre 0.4 y 0.5 indican que es aceptable realizar un análisis de factores, no obstante, se sugiere verificar el estado de las variables.
- Los valores de KMO menores a 0.4 significan que existen grandes correlaciones parciales en comparación con la suma de las correlaciones. En otras palabras, existen correlaciones generalizadas que son un gran problema para el análisis factorial, por lo que no se recomienda efectuar un análisis factorial.

El resultado de la prueba se muestra a continuación:  

```{r}
# Prueba de KMO
KMO(r = cor(est_seg))
```

De acuerdo con la salida anterior, para el modelo completo se tiene un valor de 0.57, lo que implica que la tabla de datos sea aceptable para efectuar un análisis de factores (AF), no obstante, se sugiere verificar el estado de las variables. Por otro lado, en el caso de cada variable, por ejemplo, las variables *seg_2_2023*, *seg_3_2023*,*seg_6_2023*, *seg_9_2023*, *seg_11_2023*, *seg_12_2023* y *seg_15_2023* tienen valores entre 0.6-0.7, lo que implica que para el AF estas variables son viables; las variables *seg_1_2023*, *seg_5_2023*, *seg_7_2023*, *seg_8_2023*, *seg_13_2023*, *seg_14_2023*, *seg_16_2023*, *seg_17_2023* y *seg_18_2023* tienen valores entre 0.4-0.5, por lo que para el AF estas variables son aceptables para realizar un análisis de factores, finalmente, la variable *seg_4_2023* tiene un valor de 0.3, por lo que no se recomienda efectuar un análisis factorial.

### 3.1.2 Prueba de esfericidad de Bartlett  

En la prueba de esfericidad de Bartlett se analiza si tiene sentido efectuar un análisis de factores a partir de las siguientes hipótesis: $H_0: R = I$ vs $H_a: R \ne I$, donde *R* es la matriz de correlaciones e *I* es una matriz identidad del mismo tamaño que *R*. En caso de no rechazar $H_0$, no tendría sentido realizar el análisis de factores. La regla establecida es rechazar $H_0$ si el valor p es menor a 0.05, el resultado de la prueba es el siguiente. 

```{r}
# Prueba
cortest.bartlett(est_seg)
```

Derivado de la anterior, ya que el valor p es casi 0 y éste es menor que 0.05 (0 < 0.05) se rechaza $H_0$, es decir, tiene sentido continuar con el análisis de factores. 

En conclusión, el análisis de factibilidad determina que es viable realizar el análisis de factores con la tabla de datos seleccionada.

# 4 Elección del número de factores.

## 4.1 Porcentaje de varianza explicada 

Para este criterio, por lo general se eligen el número de factores que explican de manera acumulada más del 70% de la varianza total. Para ello se obtienen los valores propios de la matriz de correlaciones y se divide entre el total de las variables de estudio. Posteriormente, se efectúa la suma acumulada para determinar el porcentaje de varianza explicada, según el número de factores seleccionados. 

```{r}
# Evitar la notación científica
options(scipen = 999)
# Se extraen los valores propios de la matriz de correlaciones
val_propios<-eigen(cor_seg)$values
# Porcentaje de varianza explicada
acumulado<-cumsum(val_propios*100/ncol(est_seg))
round(acumulado, 2)
# Gráfico de la varianza acumulada
barplot(acumulado, col = "lightblue", border = "white", names.arg = 1:17, 
        main = "Porcentaje de varianza explicada", 
        xlab = "Número de factores", ylab = "Porcentaje de varianza")
abline(h = 70, lty = 2, col = "red", lwd = 2)
box()
``` 

De acuerdo con las salidas previas, con cinco factores se supera el 70% del total de la varianza explicada (línea roja punteada del gráfico), ya que se obtiene el 75.61%, con seis factores se llega al 81.57%, y así sucesivamente. Aunque si se consideran 4 factores se acumula el 69% de la varianza explicada. Con este criterio cinco factores se deben considerar. 

## 4.2 Criterio de Kaiser

Es uno de los criterios más utilizados por los paquetes estadísticos para determinar el número adecuado de factores. Se aplica cuando se trabaja con la matriz de correlaciones muestrales *R*. Consiste en elegir el número de valores propios de *R* que sean mayores a 1. Esta regla busca que cualquier factor retenido contenga al menos la varianza de una de las variables utilizadas en el análisis. 

```{r}
# Se imprimen los valores propios a dos decimales
round(val_propios, 2)
```
 
Con base a este criterio y a la salida anterior, se deberían elegir 6 factores, ya que sus valores propios son de 5.44, 2.61, 2.25, 1.43, 1.12 y 1.01, de forma respectiva, sin embargo, el sexto factor es casi 1. Por lo que se puede considerar el tomar solo 5 factores.


## 4.3 Gráfico de sedimentación

Este criterio complementa al anterior y se basa también en el análisis de la magnitud de los valores propios, pero a partir de la tendencia que se observa en el gráfico. Se procura seleccionar un grupo reducido de factores que tengan valores propios significativamente superiores a los demás, para lo cual se identifica el punto de inflexión en la curva del gráfico de sedimentación (también referido como el codo por su semejanza con un brazo) a partir del cual la curva se transforma a una línea “plana” o relativamente recta. El gráfico para este ejercicio se presenta a continuación.


```{r, warning=FALSE}
# Gráfico de sedimentación
scree(est_seg, main = "Gráfico de sedimentación", pc=FALSE, fa=TRUE)
```

En la gráfica previa se aprecia un punto de inflexión después de 2 factores para AF, mientras que la regla de Kaiser sugiere tres.

## 4.4 Análisis paralelo

El análisis paralelo suele complementar los anteriores cuando el número de variables iniciales y factores resultantes es elevado. El procedimiento se basa en el principio de que los factores a extraer deben dar cuenta de más varianza que la esperada de manera aleatoria. El procedimiento reordena las observaciones de manera aleatoria entre cada variable y los valores propios son recalculados a partir de esta nueva base de datos aleatoriamente ordenada. Los factores con valores propios mayores a los valores aleatorios son retenidos para interpretación.


Se comparan gráficamente las dos líneas del gráfico de sedimentación, una para el análisis de los datos reales y otra para el análisis de los datos aleatorios. El número de factores se determina con la intersección de ambas gráficas. El resultado de este criterio se muestra en la próxima gráfica.  

```{r, warning=FALSE}
# Análisis paralelo
fa.parallel(est_seg, main = "Análisis paralelo")
```
De acuerdo con el análisis paralelo, sugiere conservar 3 factores en el caso de un AF y 3 componentes para un ACP. Hasta el momento los análisis sugieren considerar entre 2 y 5 factores.

# 5. Análisis factorial confirmatorio  

Para el análisis factorial confirmatorio se obtiene la solución por máxima verosimilitud de **L** (cargas) y $\mathbf{\Psi}$ (varianzas específicas) para 5 y 6 factores, considerando una rotación *varimax* para facilitar la interpretación de los factores.

## 5.1 Porcentaje de varianza explicada 

La rotación de las cargas busca mejorar la interpretación de los factores. Al rotarlos, la suma de los valores propios no se altera, pero podrían cambiar los valores propios, el porcentaje de varianza explicada y los valores de carga de cada factor. Por lo anterior, el porcentaje de varianza explicada por cada uno de los modelos propuestos se muestra a continuación.

```{r}
# Cargas estimadas rotadas para 5 factores por máxima verosimilitud
af.mv5<-factanal(est_seg, factors = 5, rotation = "varimax", n.obs = nrow(est_seg))
# Se almacenan las cargas
cargas_mv5<-af.mv5$loadings
vx5<-colSums(cargas_mv5^2)
# Se imprime la proporción de varianza explicada
rbind(`SS loadings` = vx5, `Proporción de la varianza` = vx5/nrow(cargas_mv5),
      `Varianza acumulada` = cumsum(vx5/nrow(cargas_mv5)))
```

De acuerdo al resultado previo, si se consideran 5 factores el porcentaje de la varianza acumulada es del 64.98%.

Por otra parte, el resultado para el modelo con 6 factores se presenta en seguida.

```{r}
# Cargas estimadas rotadas para 6 factores por máxima verosimilitud
af.mv6<-factanal(est_seg, factors = 6, rotation = "varimax", n.obs = nrow(est_seg))
# Se almacenan las cargas
cargas_mv6<-af.mv6$loadings
vx6<-colSums(cargas_mv6^2)
# Se imprime la proporción de varianza explicada
rbind(`SS loadings` = vx6, `Proporción de la varianza` = vx6/nrow(cargas_mv6),
      `Varianza acumulada` = cumsum(vx6/nrow(cargas_mv6)))
```

Cuando se ajusta el modelo con 6 factores, el porcentaje de la varianza acumulada es del 71.11%, por lo que el modelo con 6 factores aumenta aproximadamente el 7% el porcentaje de la varianza acumulada en comparación con el modelo de 5 factores.
Con base en el análisis previo para elegir el número de factores, se considera que se trabajarán las comunalidades y varianzas especificas con 5 y 6 factores.  

## 5.2 Estimación de cargas mediante máxima verosimilitud

Se realizaron las estimaciones de las cargas rotadas, las **comunalidades**, que es la parte de la variación de la variable que es compartida con otras variables, y las **varianzas únicas o específicas**, que es la parte de la variación de la variable que es propia de esa variable, para los modelos de 5 y 6 factores.

```{r}
# Modelo con 5 factores
# Comunalidades
comu_mv5<-diag(cargas_mv5[, 1:5] %*% t(cargas_mv5[, 1:5]))
# Unicidades o varianza específica
espe_mv5<-1 - comu_mv5

# Se unen los resultados para el modelo de 5 factores
res_mv5<-data.frame(CARGA5 = cargas_mv5[, 1:5], COM5 = comu_mv5, ESP5 = espe_mv5)
# Se imprime el resultado de las cargas rotadas
kbl(res_mv5, longtable = T, booktabs = T, digits = 3,
    caption = "Modelo con  factores por máxima verosimilitud", 
    col.names = c("F_1", "F_2", "F_3","F_4","F_5", "Comunalidad", "Varianza única"))
```


En la salida anterior se puede apreciar que, la varianza única de *seg_4_2023* (Cifra negra de delitos en la entidad federativa) es la más alta con 0.812, lo que implica que los factores no están explicando de buena forma la variabilidad de esta variable, asimismo, *seg_18_2023* (Índice de Impunidad estatal en materia penal en entidades federativas), *seg_5_2023* (Prevalencia delictiva por cada diez mil unidades económicas), *seg_8_2023* (Promedio de costos del delito contra personas, por entidad federativa), *seg_6_2023* (Cifra negra de delitos en unidades económicas), *seg_13_2023* (Tasa de policías por cada cien mil habitantes) y  *seg_9_2023* (Promedio de costos del delito en unidades económicas, por entidad federativa) cuentan con varianzas específicas de 0.766, 0.756, 0.718, 0.604, 0.552 y 0.0544, de forma respectiva.  

De manera análoga, el resultado para el modelo con 6 factores es el siguiente.

```{r}
# Modelo con 6 factores
# Comunalidades
comu_mv6<-diag(cargas_mv6[, 1:6] %*% t(cargas_mv6[, 1:6]))
# Unicidades o varianza específica
espe_mv6<-1 - comu_mv6

# Se unen los resultados para el modelo de 6 factores
res_mv6<-data.frame(CARGA6 = cargas_mv6[, 1:6], COM6 = comu_mv6, ESP6 = espe_mv6)
# Se imprime el resultado de las cargas rotadas
kbl(res_mv6, longtable = T, booktabs = T, digits = 3,
    caption = "Modelo con 6 factores por máxima verosimilitud", 
    col.names = c("F_1", "F_2", "F_3","F_4","F_5","F_6", "Comunalidad", "Varianza única"))
```

Al incluir más factores en el modelo, en algunas variables las comunalidades se incrementan, por ende, las especificidades o varianzas únicas disminuyen. De acuerdo con la tabla anterior, al incluir un factor más en el modelo, la especificidad de *seg_4_2023* pasó de 0.812 a 0.129, lo cual es una disminución importante. Por otra parte, para las variables *seg_18_2023*, *seg_5_2023*, *seg_8_2023*, *seg_6_2023*, *seg_13_2023* y *seg_9_2023*,  las varianzas únicas fueron de 0.798 (0.766 para el modelo de 5 factores), 0.749 (0.756 para el modelo de 5 factores), 0.658 (0.718 para el modelo de 5 factores),0.574 (0.604 para el modelo de 5 factores), 0.496 (0.552 para el modelo de 5 factores) y 0.525 (0.544 para el modelo de 5 factores),de manera correspondiente.
Solo en la variable *seg_4_2023* se aprecia una disminución considerable, en las demás variables la varianza única es muy similar que en el modelo de 5 factores.

## 5.3 Prueba de hipótesis para el número de factores comunes  

El supuesto de normalidad permite realizar pruebas sobre la suficiencia del modelo. Es decir, se puede probar si el número de factores *m* es suficiente para explicar la covarianza observada. Suponiendo que el modelo de *m* factores comunes tiene buen ajuste, entonces $\mathbf{\Sigma} = \mathbf{LL}'$ + $\mathbf{\Psi}$, y probar el ajuste del modelo de *m* factores comunes es equivalente a probar:
$H_0$: $\mathbf{\Sigma} = \mathbf{LL}'$ + $\mathbf{\Psi}$ (El número de factores en el modelo, es suficiente para capturar la dimensionalidad completa del conjunto de datos).
\newline $H_1$: $\mathbf{\Sigma} \ne \mathbf{LL}'$ + $\mathbf{\Psi}$ (El número de factores en el modelo, no es suficiente para capturar la dimensionalidad completa del conjunto de datos).  

La regla de decisión es rechazar $H_0$ si el valor p es menor que 0.05. Tal resultado indica que el número de factores es demasiado pequeño. Por el contrario, no rechazamos $H_0$ si el valor p excede 0.05. Tal resultado indica que es probable que haya suficientes (o mas que suficientes) factores que capturen la dimensionalidad completa del conjunto de datos. 

El resultado de la prueba para el modelo de 5 factores es el siguiente.

 Prueba de hipótesis

```{r}
# Valor p para el modelo de 5 factores
af.mv5$PVAL
```
Como el valor p es mayor a 0.05 (0.2230781>0.05) no se rechaza $H_0$, es decir, 5 factores en el modelo son suficientes para capturar la dimensionalidad completa del conjunto de datos.


```{r}
# Valor p para el modelo de 6 factores
af.mv6$PVAL
```

Como el valor p es mayor a 0.05 (0.2886158>0.05) no se rechaza $H_0$, es decir, 6 factores en el modelo son suficientes para capturar la dimensionalidad completa del conjunto de datos.

**NOTA**: Se realizó la prueba para 3 y 4 factores y en ambos casos se rechazó la hipótesis $H_0$. Las pruebas están en la sección del *ANEXO*

## 5.4 Matriz de residuales

Se realiza la comparación de las matrices de correlación, en el cual se obtiene la diferencia entre la matriz de correlaciones real y la matriz reproducida mediante *m* factores, si el número de factores *m* es adecuado, la diferencia debe ser cercana a 0. Para ello se calculan las correlaciones reproducidas, que es la matriz de correlaciones de las variables originales que resultaría suponiendo que es correcto el número de factores retenidos. Asimismo, se calcula la matriz de correlaciones residuales, que es la matriz de las diferencias entre la matriz de correlaciones reproducida y la matriz de correlaciones reales. Si el número de factores *m* es adecuado, se esperaría que la matriz de correlaciones residuales tenga todas sus entradas cercanas a 0. 

El resultado para el modelo de 5 factores es el siguiente.

```{r}
# Matriz reproducida (5 factores)
mat_rep_mv5<-(cargas_mv5[, 1:5] %*% t(cargas_mv5[, 1:5])) + diag(espe_mv5)
# Matriz de residuales (5 factores)
mat_res_mv5<-cor_seg - mat_rep_mv5
# Gráficos de correlaciones
corrplot(mat_res_mv5, order = "original", tl.col = "black", tl.cex = 0.5, 
         mar = c(0, 0, 2, 0), title = "Matriz de residuales para 5 factores por MV")
```

Con base a la gráfica previa, se aprecia que la mayoría de las celdas son cercanas a 0, ya que no se observan círculos de color intenso. Los residuales más grandes que pueden distinguirse son para los pares *(seg_6_2023, seg_8_2023)*, *(seg_8_2023, seg_18_2023)*, *(seg_4_2023, seg_18_2023)*, *(seg_4_2023, seg_6_2023)* y *(seg_4_2023, seg_6_2023)* cuyos valores son: -0.2692, 0.2494, 0.1859, 0.1756 y 0.1746, de forma respectiva. En conclusión, con cinco factores se realiza un buen ajuste, aunque se tienen algunos residuales con valores moyor a 0 pero menor a 0.3.

A continuación, se muestra el resultado para el modelo de 6 factores.  

```{r}
# Matriz reproducida (6 factores)
mat_rep_mv6<-(cargas_mv6[, 1:6] %*% t(cargas_mv6[, 1:6])) + diag(espe_mv6)
# Matriz de residuales (6 factores)
mat_res_mv6<-cor_seg - mat_rep_mv6
# Gráficos de correlaciones
corrplot(mat_res_mv6, order = "original", tl.col = "black", tl.cex = 0.5, 
         mar = c(0, 0, 2, 0), title = "Matriz de residuales para 6 factores por MV")
```

De acuerdo con la salida anterior, la gran mayoría de las celdas son cercanas a 0, puesto que no se observan círculos de color intenso. Los residuales que más puede distinguirse son para el par de variables *(seg_5_2023, seg_6_2023)*, *(seg_8_2023, seg_18_2023)*, *(seg_5_2023, seg_8_2023)* y *(seg_5_2023, seg_9_2023)*, con valores de: 0.2205,0.1747,-0.1509 y 0.1405, por lo que con 6 factores prácticamente los residuales son cercanos a 0. 

Hasta el momento, con el modelo de 5 factores se explica el 64.98% de la varianza, la prueba de hipótesis para el número de factores comunes determinó que 5 factores son suficientes, y la matriz de residuales arrojó 5 pares de valores considerables. Por otra parte, con el modelo de 6 factores se explica el 71% de la varianza, de igual forma, la prueba de hipótesis sugirió que 6 factores son suficientes, y la matriz de residuales arrojó valores muy cercanos a cero. 

## 5.5 Interpretación de los factores  

La interpretación de las cargas de los modelos, generalmente se explican en términos de las cargas principales en cada factor. Para ello se realizó el siguiente gráfico para el modelo de 5 factores.  

```{r}
# Gráfico para interpretaciones de los factores
fa.diagram(af.mv5$loadings, main = "Modelo de 5 factores", cex = 1, rsize = 0.7)
```

1.- Para el diagrama con 5 factores, vemos que el **factor 1** se asocia con las variables:*seg_2_2023*,*seg_3_2023*, *seg_9_2023*, *seg_13_2023* y *seg_5_2023*, estas variables están relacionadas con: Incidencia delictiva por cada cien mil habitantes, Prevalencia delictiva por cada cien mil habitantes, Promedio de costos del delito en unidades económicas, por entidad federativa, Tasa de policías por cada cien mil habitantes y Prevalencia delictiva por cada diez mil unidades económicas.

2.- El **factor 2** está asociado con las variables: *seg_1_2023*, *seg_7_2023* y *seg_8_2023*, estas variables son: Tasa de homicidios por cada cien mil habitantes, Índice de Riesgo por Crimen Organizado y Promedio de costos del delito contra personas, por entidad federativa.  

3.- El **factor 3** está asociado con las variables: *seg_16_2023*, *seg_15_2023*, *seg_14_2023* y *seg_4_2023*, estas variables son: Desconfianza en ministerios públicos, Desconfianza en jueces, Desconfianza en la policía estatal y municipal y Cifra negra de delitos en la entidad federativa con carga negativa.

4.- El **factor 4** está asociado con las variables: *seg_11_2023* y *seg_12_2023*, estas variables son: Porcentaje de personas que consideran insegura su entidad federativa y Porcentaje de personas que cambió algún hábito por temor a sufrir algún delito.

5.- El **factor 5** está asociado con las variables: *seg_17_2023*, *seg_6_2023* y *seg_18_2023*, estas variables son: Variación porcentual del índice de conflictividad en la entidad federativa, Cifra negra de delitos en unidades económicas y Índice de Impunidad estatal en materia penal en entidades federativas. 

```{r}
# Gráfico para interpretaciones de los factores
fa.diagram(af.mv6$loadings, main = "Modelo de 6 factores", cex = 1, rsize = 0.7)
```

1.- Para el diagrama con 6 factores, vemos que el **factor 1** se asocia con las variables:*seg_16_2023*,*seg_15_2023*, *seg_14_2023* y *seg_13_2023*, estas variables están relacionadas con: Desconfianza en ministerios públicos, Desconfianza en jueces, Desconfianza en la policía estatal y municipal y Tasa de policías por cada cien mil habitantes.  

2.- El **factor 2** está asociado con las variables: *seg_7_2023*, *seg_1_2023* y *seg_8_2023*, estas variables son: Índice de Riesgo por Crimen Organizado, Tasa de homicidios por cada cien mil habitantes y Promedio de costos del delito contra personas, por entidad federativa.  

3.- El **factor 3** está asociado con las variables: *seg_2_2023*, *seg_3_2023*, *seg_9_2023* y *seg_5_2023*, estas variables son: Incidencia delictiva por cada cien mil habitantes, Prevalencia delictiva por cada cien mil habitantes, Promedio de costos del delito en unidades económicas, por entidad federativa y Prevalencia delictiva por cada diez mil unidades económicas. 

4.- El **factor 4** está asociado con las variables: *seg_17_2023*, *seg_6_2023* y *seg_18_2023*, estas variables son: Variación porcentual del índice de conflictividad en la entidad federativa, Cifra negra de delitos en unidades económicas y Índice de Impunidad estatal en materia penal en entidades federativas.

5.- El **factor 5** está asociado con las variables: *seg_12_2023* y *seg_11_2023*, estas variables son: Porcentaje de personas que cambió algún hábito por temor a sufrir algún delito y Porcentaje de personas que consideran insegura su entidad federativa

6.- El **factor 6** está asociado con la variable: *seg_4_2023*, esta variable es: Cifra negra de delitos en la entidad federativa.

Con base en el análisis anterior, se sugiere considerar 5 factores en el análisis, a pesar de que se explica un 64% de la varianza, sin embargo, las demás pruebas consideraron que con 5 factores el análisis es pertinente. Además, siguiendo el principio de parsimonia estadística es más sencillo interpretar 5 factores que 6.


## 5.6 Estimación de factor scores 

Para la estimación de los vectores de puntajes factoriales (factor scores), se implementaron los métodos más consistentes y utilizados, los cuales son el de mínimos cuadrados ponderados (Bartlett) y el método por regresión (Thompson). El método por mínimos cuadrados ponderados supone que el vector de valores de los factores para cada observación es un parámetro a estimar. El método por regresión supone que estos vectores son variables aleatorias. El resultado de la estimación de los factor scores para el modelo de 5 factores se presenta a continuación.

Cálculo de factor scores mediante enfoque de mínimos cuadrados

```{r}
# Cálculo de factor scores mediante enfoque de mínimos cuadrados
scores_min<-factanal(est_seg, factors = 5, scores = "Bartlett")$scores
# Identificadores de los registros
ENT<-formatC(datos$cve, width = 2, format = "d", flag = "0")
# Se crea el data frame con los resultados
res_score1<-data.frame(ID = paste0(ENT), scores_min)
# Se ordenan los resultados por ID
res_score1<-res_score1[order(res_score1$ID), ]

# Se imprime el resultado por mínimos cuadrados
kbl(res_score1[1:32, ], longtable = T, booktabs = T, escape = FALSE, digits = 3,
    col.names = c("Entidad", "Score 1", "Score 2", "Score 3", "Score 4", "Score 5"), 
    row.names = FALSE,
    caption = "Factor Scores estimados por Mínimos cuadrados") %>%
add_header_above(c(" ", "Mínimos cuadrados" = 5)) %>% 
kable_styling(latex_options = c("repeat_header"))   
```
Cálculo de factor scores mediante enfoque de regresión

```{r}
# Cálculo de factor scores mediante enfoque de regresión
scores_reg<-factanal(est_seg, factors = 5, scores = "regression")$scores
# Identificadores de los registros
ENT<-formatC(datos$cve, width = 2, format = "d", flag = "0")
# Se crea el data frame con los resultados
res_score2<-data.frame(ID = paste0(ENT), scores_reg)
# Se ordenan los resultados por ID
res_score2<-res_score2[order(res_score2$ID), ]

# Se imprime el resultado por regresión
kbl(res_score2[1:32, ], longtable = T, booktabs = T, escape = FALSE, digits = 3,
    col.names = c("Entidad", "Score 1", "Score 2", "Score 3", "Score 4", "Score 5"), 
    row.names = FALSE,
    caption = "Factor Scores estimados por regresión") %>%
add_header_above(c(" ", "Regresión" = 5)) %>% 
kable_styling(latex_options = c("repeat_header"))   
```

Cálculo de las diferencias

```{r}
# Cálculo de las diferencias
diferencia<-scores_min - scores_reg
# Identificadores de los registros
ENT<-formatC(datos$cve, width = 2, format = "d", flag = "0")
# Se crea el data frame con los resultados
res_score3<-data.frame(ID = paste0(ENT), diferencia)
# Se ordenan los resultados por ID
res_score3<-res_score3[order(res_score3$ID), ]

# Se imprime el resultado de las diferencias
kbl(res_score3[1:32, ], longtable = T, booktabs = T, escape = FALSE, digits = 3,
    col.names = c("Entidad", "Score 1", "Score 2", "Score 3", "Score 4", "Score 5"), 
    row.names = FALSE,
    caption = "Diferencias") %>%
add_header_above(c(" ", "Diferencias" = 5)) %>% 
kable_styling(latex_options = c("repeat_header"))  
# Cálculo del promedio de las diferencias
round(apply(diferencia, 2, mean), 3)
# Cálculo de la desviación estándar de las diferencias
round(apply(diferencia, 2, sd), 3)
```
La tablas anteriores contienen las estimaciones de los factor scores para las 32 entidades. Asimismo, se muestra el promedio de las diferencias (prácticamente 0) y sus desviaciones estándar (cercanas a 0) entre los enfoques de mínimos cuadrados y regresión. Se puede apreciar que las diferencias son mínimas, así que las interpretaciones que se den sobre las entidades serán aplicables a ambos enfoques. Por lo anterior, se determinó trabajar con los factor scores estimados mediante regresión.


## 5.7 Interpretación de resultados

Se considera distinguir a las 32 entidades federativas. Para facilitar la interpretación de los resultados, se realizaron las diferentes parejas posibles de los factor scores, además de dividir la gráfica en 4 cuadrantes. Asimismo, se identificaron los puntos extremos de cada cuadrante (representados con color rojo) para analizar su información. El resultado se muestra a continuación.

Se realiza el gráfico del Factor 1 vs Factor 2 (Opción 2)

```{r}

plot(res_score2[, 2], res_score2[, 3],main="Factor 1 vs Factor 2",xlim=c(-2.5,2.5),ylim=c(-3,3),
 xlab="Factor 1",ylab="Factor 2",type="n",lwd=2)
text(res_score2[, 2],res_score2[, 3],
 labels=abbreviate(row.names(res_score2),minlength=8),cex=0.6,lwd=2, col = "blue")
abline(v = 0, h = 0, lty =2, col ="red")

```

Para la interpretación de las gráfica se recuerda que el factor 1 está relacionado con el nivel de criminalidad, promedio del costo del delito por unidad económica y la tasa de policías, mientras el factor 2 se relaciona con la tasa de homicidios, índice de riesgo por el crimen organizado y el promedio de costos por delitos a nivel persona, finalmente, el factor 3 está relacionado con el desempeño institucional (desconfianza en autoridades) y cifra negra delitos a nivel persona.
De esta manera, la entidad 9 (CDMX) es la que tiene el valor más alto en el factor 1, es decir, tiene alto nivel de criminalidad, en cambio el con respecto al factor 2 tiene valor cercano a la media nacional. La entidad 15 (EDOMEX) tiene un comportamiento muy parecido al de CDMX.
La entidad 2 (Baja California) también registra valores altos en el factor 1 y con respecto al factor 2, registra valores un poco más arriba que la media nacional.
Para la entidad 32 (Zacatecas) se registra valores por debajo de la media nacional en el factor 1sin embargo, en el factor 2 es la entidad con los valores más altos.
Par las entidades 16 (Michoacán) y 7 (Chiapas) registran los valores más pequeños en el factor 1, sin embargo, la primera entidad mencionada registra valores por arriba del promedio en el factor 2, mientras que Chiapas registra valores también muy pequeños en el factor 2.
Finalmente, para el caso de la entidad 31 (Yucatán) registra un valor cercano a cero en factor 1, sin embargo, el valor del factor 2 es muy pequeño, casi a la misma altura que la entidad de Chiapas. 


Se realiza el gráfico del Factor 1 vs Factor 3

```{r}

plot(res_score2[, 2], res_score2[, 4],main="Factor 1 vs Factor 3",xlim=c(-2.5,2.5),ylim=c(-3.5,3.5),
 xlab="Factor 1",ylab="Factor 3",type="n",lwd=2)
text(res_score2[, 2],res_score2[, 4],
 labels=abbreviate(row.names(res_score2),minlength=8),cex=0.6,lwd=2, col = "blue")
abline(v = 0, h = 0, lty =2, col ="red")
```
En la siguiente gráfica se muestra que la entidad 9 (CDMX) se encuentra en el cuadrante II reporta los valores más altos es ambos factores, lo que se interpreta como altos niveles de criminalidad y alta desconfianza en las autoridades analizadas, un comportamiento similar lo tiene la entidad 15 (EDOMEX).
Por otro lado, la entidad 19 (Nuevo León) que se reporta en el cuadrante IV, registra un valor medianamente alto del factor 1 y un valor bajo del factor 3, es decir, existe hay nivel de criminalidad, sin embargo, la desconfianza en las autoridades analizadas es baja.
En el cuadrante 1 de la gráfica se encuentra la entidad de 16 (Michoacán de Ocampo),en donde registra valores bajos en el factor 1 (nivel de criminalidad principalmente) sin embargo, la desconfianza en las autoridades es muy alta muy parecida a la de la CDMX.
En el extremo del cuadrante III, se encuentra la entidad 7 (Chiapas), en donde reporta un valor bajo en el factor 1 y un valor menor al promedio nacional en el factor 3.
Las entidades  (11, 14 y 2) Guanajuato, Jalisco y Baja California reportan valores considerables (aunque no los máximos) en el factor 1 y en el factor 3.


Se realiza el gráfico del Factor 2 vs Factor 3

```{r}

plot(res_score2[, 3], res_score2[, 4],main="Factor 2 vs Factor 3",xlim=c(-3,3),ylim=c(-3.5,3.5),
 xlab="Factor 2",ylab="Factor 3",type="n",lwd=2)
text(res_score2[, 3],res_score2[, 4],
 labels=abbreviate(row.names(res_score2),minlength=8),cex=0.6,lwd=2, col = "blue")
abline(v = 0, h = 0, lty =2, col ="red")
```

En el biplot anterior, se muestra la relación del factor 2 y factor 3, por ejemplo, en el cuadrante I, se observa que la entidad 9 (CDMX), en donde se registra un valor pequeño en el factor 2(Tasa de homicidio, índice de riesgo por crimen organizado y promedio de cotos del delitos a nivel persona), pero un valor muy alto en el factor 3 (desconfianza en las y cifra negra a nivel persona.)
En el cuadrante II, la entidad 2 (Baja California) es la que se encuentra en el extremo, registrando valor alto en el factor 2, sin embargo, en el factor 3 registra un valor un poco arriba del promedio del nacional.

En el cuadrante III, se encuentra la entidad 19 (Nuevo León) que registra un nivel por debajo del promedio en el factor 2 y el valor más bajo en el factor 3. Es decir, hay registros de la tasa de homicidios, índice de riesgo por crimen organizado, sin embargo, la desconfianza en las autoridades es baja.
Finalmente, en el extremo del cuadrante IV, se encuentra la entidad 32 (Zacatecas),registrando valor alto en el factor 2, sin embargo, en el factor 3 registra un valor un poco por debajo del promedio nacional.


## 5.8 Cálculo del subíndice de seguridad

Ahora, se obtienen los factor scores para crear un data frame  y obtener la primera opción del subíndice de seguridad.

Opción 1. Como se observa a continuación, se utiliza forma de cálculo que se describe en la *NOTA 1*, solo que en este caso, se obtienen los  factor scores por el método de análisis de factores.

```{r}
#Se extraen los 5 factor scores
bd_score2<-res_score2
#Se convierte bd_factor_score en data frame 
bd_score2<- as.data.frame(bd_score2)
bd_score2$media_seg<- apply(bd_score2[,c(2:6)],1,mean)
val_min<- min(bd_score2$media_seg)# -0.6837303 
bd_score2$dif<-(bd_score2$media_seg-val_min)
val_max<- max(bd_score2$dif) #1.808364
bd_score2$ind_seg<-(bd_score2$dif/val_max)*100 #La variable entidad ya está incluída
```

Opción 2. Como se observa a continuación, se utiliza forma de cálculo que se describe en la *NOTA 2*, solo que en este caso, se obtienen los  factor scores por el método de análisis de factores.

```{r}
#Se extraen los 5 factor scores
bd_score3<-res_score2
#Se convierte en data frame
bd_score3<- as.data.frame(bd_score3)
val_min1<- min(bd_score3$Factor1)
val_min2<- min(bd_score3$Factor2)
val_min3<- min(bd_score3$Factor3)
val_min4<- min(bd_score3$Factor4)
val_min5<- min(bd_score3$Factor5)
bd_score3$dif1<-(bd_score3$Factor1-val_min1)
bd_score3$dif2<-(bd_score3$Factor2-val_min2)
bd_score3$dif3<-(bd_score3$Factor3-val_min3)
bd_score3$dif4<-(bd_score3$Factor4-val_min4)
bd_score3$dif5<-(bd_score3$Factor5-val_min5)
val_max1<- max(bd_score3$dif1)
val_max2<- max(bd_score3$dif2)
val_max3<- max(bd_score3$dif3)
val_max4<- max(bd_score3$dif4)
val_max5<- max(bd_score3$dif5)
bd_score3$ind_seg1<-(bd_score3$dif1/val_max1)*100
bd_score3$ind_seg2<-(bd_score3$dif2/val_max2)*100
bd_score3$ind_seg3<-(bd_score3$dif3/val_max3)*100
bd_score3$ind_seg4<-(bd_score3$dif4/val_max4)*100
bd_score3$ind_seg5<-(bd_score3$dif5/val_max5)*100
bd_score3$ind_seg_f<-apply(bd_score3[,c(12:16)],1,mean)
```


# 6. Análisis de componentes principales versión 2

A continuación, se realiza el análisis de Componentes principales quitando algunas variables, para ello, se revisó a las variables que no tuviera alta correlación con las demás variables y fueron las que en este nuevo análisis no se consideraron.

La variables que se eliminaron fueron:

 1. seg_4_2023: Cifra negra de delitos en la entidad federativa
 2. seg_5_2023: Prevalencia delictiva por cada diez mil unidades económicas
 3. seg_6_2023: Cifra negra de delitos en unidades económicas
 4. seg_8_2023: Promedio de costos del delito contra personas, por entidad federativa
 5. seg_18_2023: Índice de Impunidad estatal en materia penal en entidades federativas 
 
De la base original "datos", se crea la base "seg1" la cual no considera a las variables antes mencionadas.

```{r}
seg1 <- datos[, -c(1,5,6,7,9,11,19,20)]
#Se estandarizan los datos
est_seg1<- scale(seg1, center = TRUE, scale = TRUE)
#Se realiza el gráfico de cajas para ver a las variables estandarizadas.
boxplot(est_seg1, col ="#FEBE8C", main="Datos estandarizados")
```

## 6.1 Técnica de Análisis de Componentes Principales.

Se aplicará un análisis de componentes principales para reducir la dimensionalidad de los datos, esto para una mejor interpretación de qué es lo que explica al fenómeno de interés, es decir, se busca una relación entre las variables estudiadas de seguridad pública.

Para que un PCA tenga sentido, es necesario que exista una alta correlación entre los predictores, por lo que se explorará a continuación.

```{r}
#seleccionamos los datos numericos
cor_seg1 <-cor(est_seg1)
#Gráfica de correlaciones entre variables
corrplot(cor_seg1, tl.cex = 1, tl.col = "black", mar = c(0, 0, 0, 0))
```

Derivado de la gráfica previa, se observa que en efecto sí existen correlaciones altas entre algunas variables. La correlación mide la fuerza y dirección de la interacción entre dos variables, suponiendo relación lineal.
Por ejemplo, la variable *seg_1_2023* tiene correlación positiva fuerte con la variable seg_7_2023, le sigue la seg_11_2023, seg_12_2023 y seg_14_2023.
En general algunas variables tiene correlaciones fuertes positivas, pero también se observan ciertas correlaciones negativas entre algunas variables.  
Finalmente, se calcula el determinante, se busca que el determinante se cercano a cero, pues implicaría un alta multicolinalidad entre variables.

```{r}
det_seg1<-det(cor_seg1) #Cálculamos determinante, como es cercano a cero, quiere decir que hay correlación.
det_seg1 #0.00000606635
```

Como el determinante es 0.00000606635, cercano a cero, quiere decir que hay correlación.
Derivado de lo anterior, el análisis de componentes principales es factible.

```{r}
### PCA

seg.pc1 <- princomp(est_seg1)
summary(seg.pc1)
```
Uno de los criterios para escoger el número adecuado de componentes principales, es cuidar que la varianza acumulada que expliquen, sea de por lo menos 70%, y de acuerdo con la salida del modelo, se necesitan tres componentes, en el ejemplo se acumula el 76.4% de la varianza del modelo. 

Otros criterios son el del codo, que a partir del gráfico de sedimentación, se tiene que ubicar el cambio de trayectoria más abrupta, y en ese punto, será el número de componentes adecuado, otro es el de Kaiser, que indica conservar aquellas componentes cuyos valores propios superen a la unidad.


```{r}
scree(est_seg1, pc=TRUE, fa=FALSE)
```
Según el gráfico anterior, el criterio del codo sugiere 4 componentes, mientras que la regla de Kaiser sugiere tres. En este caso, los criterios probados indican un número diferente de componentes, sin embargo, se va a priorizar la parsimonía del modelo, es decir, se van a elegir tres componentes pues el porcentaje de varianza acumulada es muy aceptable.

```{r}
# Se eligen 3 componentes
#Se cachan los vectores propios

pc1 <- seg.pc1$loadings[,1]
pc2 <- seg.pc1$loadings[,2]
pc3 <- seg.pc1$loadings[,3]

#Se realizan las gráficas

plot(pc1, ylim = c(-1, 1.5), type = "l", lwd=2)
lines(pc2, col="blue", lwd=2)
lines(pc3, col="green", lwd=2)
abline(h=0, lty=2, col="red")
legend(x="topright", legend = c("PC1", "PC2","PC3"),
       fill = c("black", "blue", "green"))
```
En la gráfica se muestra el comportamiento de los primeros 3 componentes principales respecto y la relación con las variables originales.

Según las cargas de los componentes, por lo menos visualmente hay algunas que destacan en alguna variable.

1. En el primer componente hay 7 variables que tienen pesos similares y son los más altos, rondando en valores de 0.3. De hecho, las cargas del componente 1 tienen un comportamiento constante.
2. Para el segundo componente hay 2 variables con cargas positivas elevadas las cuales son seg_7_2023, seg_1_2023 con cargas de 0.518 y 0.502 respectivamente, además, en tercer lugar quedó la variable seg_13_2023 con carga de -0.344 y ésta última hace contraste con los componentes antes mencionados.
3. En el componente 3, se observa que la variable con mayor carga la tiene la variable seg_17_2023 con un valor de -0.685,y luego, la carga de la variable seg_13_2023 con el siguiente valor mayor es -0.328. Por otro lado la variable seg_3_2023 registra una carga de 0.309, en general, se considera que el comportamiento de este componente es con una tendencia decreciente.


Para aterrizar lo anterior, se dibuja el diagrama donde se indica qué variables son las que explican a cada uno de los componentes.

```{r}
fa.diagram(seg.pc1$loadings[,1:3], main = "Componentes principales")
```
Según el diagrama anterior, el componente uno está definido por las variables:seg_15_2023, seg_14_2023, seg_16_2023, seg_2_2023, seg_11_2023, seg_3_2023 y seg_12_2023, todas en sentido positivo y con una carga muy similar, este componente puede estar explicando el desempeño institucional,percepción de inseguridad, cambio de rutinas y Nivel de criminalidad (incidencia delictiva). El componente dos se define por las variables seg_7_2023, seg_1_2023 y seg_13_2023, éste último con carga negativa, que se refiere a temas con el crimen organizado, Nivel de criminalidad (Tasa de homicidios) y la tasa de policías. El componente tres se define por las variables seg_17_2023 con carga negativa, relacionados con el desempeño institucional en particular con la variación porcentual del índice de conflictividad. Por otro lado, la variable seg_9_2023 Promedio de costos del delito en unidades económicas, por entidad federativa no fue relacionada con ningún componente.

Ahora, se pueden graficar los scores (a nivel dato) por pares de los componentes principales.
Por temas de interpretación, solo se graficarán los scores de los dos primeros componentes principales.

## 6.2 Gráficas de los scores

Ahora se graficaran los tres scores por pares (comp1 vs comp2), (comp1 vs comp3) y (comp2 vs comp3).

Primero se extraen los puntajes de las observaciones (scores).
```{r}
#Se extraen los scores
score1_v2<-seg.pc1$scores[,1]
score2_v2<-seg.pc1$scores[,2]
score3_v2<-seg.pc1$scores[,3]
```

Recordando que el componente 1 está relacionado con nivel de criminalidad, percepción de inseguridad y cambio de rutinas y con el desempeño institucional, principalmente relacionado con la desconfianza de la autoridades, el componente 2 con la tasa de homicidio, índice de riesgo por crimen organizado y la tasa de policías por cada cien mil hab. y finalmente, el factor 3 está relacionado con la variable: Variación porcentual del índice de conflictividad en la entidad federativa.


Se realiza el gráfico score 1 vs score 2

```{r}
#Se realiza el gráfico
plot(score1_v2,score2_v2,main="Biplot score 1 vs score 2",xlim=c(-5,7),ylim=c(-5,4.5),
 xlab="score 1",ylab="score 2",type="n",lwd=2)
text(score1_v2,score2_v2,
 labels=abbreviate(row.names(seg),minlength=8),cex=0.6,lwd=2, col = "blue")
abline(v = 0, h = 0, lty =2, col ="red")

```

En este ejemplo, el componente 1 un poco difícil de explicar por todas las variables asociadas a él.
La gráfica muestra que en el cuadrante I, están las entidades con valores bajos respecto al componente 1, pero alrededor de la media nacional con respecto al componente 2.
En el extremo superior del cuadrante II se encuentra la entidad de 32 (Zacatecas), en donde reporta valores un poco mayores a media nacional en el componente 1, sin embargo, en el componente 2 es la entidad que registra el valor más alto.
La entidad 23 (Quinta Roo), registra un valor alto en el componente 1 y un valor por arriba de la media nacional en el componente2.
En el extremo inferior del cuadrante III, cayó la entidad 31 (Yucatán), que es la que registra un valor pequeño en el componente 1 y también registra un valor pequeño en el componente 2.
Y finalmente, para el cuadrante IV, en el extremo inferior derecho, se encuentra la entidad 9 con valores altos en el componente 1 y muy bajos en el componente 2, se puede interpretar que es la entidad con mayores tasas de incidencia, mayor desconfianza en la autoridades, en cambio, tiene una tasa de homicidios baja, una índice de riesgo por crimen organizado también bajo y la tasa de policías alta (está en sentido contrario). Un comportamiento similar registra la entidad 15 (EDOMEX).



Se realiza el gráfico score 1 vs score 3 

```{r}
#Se realiza el gráfico

plot(score1_v2,score3_v2,main="Biplot score 1 vs score 3",xlim=c(-5,7),ylim=c(-3.5,3.5),
 xlab="score 1",ylab="score 3",type="n",lwd=2)
text(score1_v2,score3_v2,
 labels=abbreviate(row.names(seg),minlength=8),cex=0.6,lwd=2, col = "blue")
abline(v = 0, h = 0, lty =2, col ="red")
```

En el gráfico se muestra la relación de que tienen los componentes 1 y 3 con base en las entidades.

En el extremo superior del cuadrante I, se encuentra la entidad 19 (Nuevo León) que reporta valores por debajo del nivel nacional del componente 1, además es una entidad que registra una variación porcentual del índice de conflictividad en la entidad federativa casi nula.
Con respecto al cuadrante II, en el extremo se encuentra la entidad 15 (EDOMEX), con un valor alto en el componente 1 y un valor similar al de Nuevo León del componente 3.

En el cuadrante III, se encuentra la entidad 31 (Yucatán), en donde registra un valor muy pequeño en el componente 1 y un valor parecido al valor nacional en el componente 3, es decir, si hay variación porcentual del índice de conflictividad. En ese mismo cuadrante está la entidad 4 (Campeche), en donde registra un valor por debajo del nacional en el componente 1, sin embargo, registra una variación porcentual del índice de conflictividad considerable.
En el cuadrante IV, se encuentra la entidad 32 (Zacatecas), en donde registra un valor por arriba del nacional para el componente 1 y una variación porcentual del índice de conflictividad (componente 3) considerable. Finalmente, la entidad 23 (Quintana Roo) tiene un comportamiento similar al de Zacatecas con respecto al componente 3.


Se realiza el gráfico score 2 vs score 3

```{r}
#Se realiza el gráfico

plot(score2_v2,score3_v2,main="Biplot score 2 vs score 3",xlim=c(-5,4.5),ylim=c(-3.5,3.5),
 xlab="score 2",ylab="score 3",type="n",lwd=2)
text(score2_v2,score3_v2,
 labels=abbreviate(row.names(seg),minlength=8),cex=0.6,lwd=2, col = "blue")
abline(v = 0, h = 0, lty =2, col ="red")
```

En gráfica anterior se muestra la relación del componente 2 y 3.

Por ejemplo, la entidad (9) CDMX, registra una de los valores más bajos en el componente 2 y y también, una porcentual del índice de conflictividad por arriba de la del valor nacional.
Luego, en la parte superior del eje Y, está la entidad 19 (Nuevo León), es decir, el componente 2 está por el que se reporta a nivel nacional, sin embargo, el componente 3 que tiene que ver con la variación porcentual del índice de conflictividad es una de las entidades en el top con variaciones casi nulas.
Finalmente, en el cuadrante IV, en el extremo inferior derecho se encuentra la entidad 32 (Zacatecas), en donde se registra un valor alto en el componente 2 y también una de las entidaes que registran mayor variación porcentual del índice de conflictividad referente al componente 3.


## 6.3 Cálculo del subíndice de seguridad de los scores por componentes principales (versión 2).

Se obtienen los scores para crear un data frame para hacer la escala del 0-100.

Opción 1. Como se observa a continuación, se utiliza forma de cálculo que se describe en la *NOTA 1*, solo que en este caso se contemplan 3 scores.

```{r}
bd_score4<-cbind(score1_v2,score2_v2,score3_v2)
#Se convierte en data frame 
bd_score4<- as.data.frame(bd_score4)
bd_score4$media_seg<- apply(bd_score4,1,mean)
val_min<- min(bd_score4$media_seg)# -2.032756
bd_score4$dif<-(bd_score4$media_seg-val_min)
val_max<- max(bd_score4$dif)
bd_score4$ind_seg<-(bd_score4$dif/val_max)*100
#bd_score4 #Es el data frame construido, la última columna "ind_seg" es el índice de seguridad.
#Finalmente se agrega la variable entidad a la base final
bd_score4$ID<-c("01","02","03","04","05","06","07","08","09",
                "10","11","12","13","14","15","16","17","18",
                "19","20","21","22","23","24","25","26","27",
                "28","29","30","31","32")
```

Opción 2. Como se observa a continuación, se utiliza forma de cálculo que se describe en la *NOTA 2*,solo que en este caso se contemplan 3 scores.

```{r}
bd_score5<-cbind(score1_v2,score2_v2,score3_v2)
#Se convierte en data frame 
bd_score5<- as.data.frame(bd_score5)
val_min1<- min(bd_score5$score1_v2)
val_min2<- min(bd_score5$score2_v2)
val_min3<- min(bd_score5$score3_v2)
bd_score5$dif1<-(bd_score5$score1_v2-val_min1)
bd_score5$dif2<-(bd_score5$score2_v2-val_min2)
bd_score5$dif3<-(bd_score5$score3_v2-val_min3)
val_max1<- max(bd_score5$dif1)
val_max2<- max(bd_score5$dif2)
val_max3<- max(bd_score5$dif3)
bd_score5$ind_seg1<-(bd_score5$dif1/val_max1)*100
bd_score5$ind_seg2<-(bd_score5$dif2/val_max2)*100
bd_score5$ind_seg3<-(bd_score5$dif3/val_max3)*100
bd_score5$ind_seg_f<-apply(bd_score5[,c(7:9)],1,mean)
#Finalmente se agrega la variable entidad a la base final
bd_score5$ID<-c("01","02","03","04","05","06","07","08","09",
                "10","11","12","13","14","15","16","17","18",
                "19","20","21","22","23","24","25","26","27",
                "28","29","30","31","32")
```

# 7. ANÁLISIS DE FACTORES (Versión 2)

## 7.1 Análisis de factibilidad  

Antes de dar inicio con la implementación del modelo de factores como tal, es importante analizar la factibilidad de los datos, para ello se analiza la matriz de correlaciones, el cálculo del determinante de la matriz anterior y se generan las pruebas de Kaiser Meyer Olkin (KMO) y de esfericidad de Bartlet.

### 7.1.1 Prueba de Kaiser Meyer Olkin  

La prueba de Kaiser-Meyer-Olkin (KMO), es una medida de la idoneidad de los datos para el análisis factorial. Mide la adecuación del muestreo para cada variable en el modelo y para el modelo completo. La estadística es una medida de la proporción de varianza entre variables que podrían ser varianza común.

La prueba de KMO devuelve valores entre 0 y 1. Una regla general para interpretar la estadística es la siguiente: 

- Los valores de KMO entre 0.8 y 1 indican que el muestreo es perfecto para realizar un análisis de factores.
- Los valores entre 0.6 y 0.7 señalan que es viable realizar un análisis de factores.
- Los valores de KMO entre 0.4 y 0.5 indican que es aceptable realizar un análisis de factores, no obstante, se sugiere verificar el estado de las variables.
- Los valores de KMO menores a 0.4 significan que existen grandes correlaciones parciales en comparación con la suma de las correlaciones. En otras palabras, existen correlaciones generalizadas que son un gran problema para el análisis factorial, por lo que no se recomienda efectuar un análisis factorial.

El resultado de la prueba se muestra a continuación:  

```{r}
# Prueba de KMO
KMO(r = cor(est_seg1))
```

De acuerdo con la salida anterior, para el modelo completo se tiene un valor de 0.59, lo que implica que la tabla de datos sea aceptable para efectuar un análisis de factores (AF), no obstante, se sugiere verificar el estado de las variables. Por otro lado, en el caso de cada variable, por ejemplo, las variables *seg_2_2023*, *seg_3_2023*, *seg_9_2023*, *seg_11_2023*, *seg_12_2023* y *seg_15_2023* tienen valores entre 0.6-0.7, lo que implica que para el AF estas variables son viables; las variables *seg_1_2023*, *seg_7_2023*, *seg_13_2023*, *seg_14_2023* y *seg_16_2023* tienen valores entre 0.4-0.5, por lo que para el AF estas variables son aceptables para realizar un análisis de factores, finalmente, la variable *seg_17_2023* tiene un valor de 0.23, por lo que no se recomienda efectuar un análisis factorial.

### 7.1.2 Prueba de esfericidad de Bartlett  

En la prueba de esfericidad de Bartlett se analiza si tiene sentido efectuar un análisis de factores a partir de las siguientes hipótesis: $H_0: R = I$ vs $H_a: R \ne I$, donde *R* es la matriz de correlaciones e *I* es una matriz identidad del mismo tamaño que *R*. En caso de no rechazar $H_0$, no tendría sentido realizar el análisis de factores. La regla establecida es rechazar $H_0$ si el valor p es menor a 0.05, el resultado de la prueba es el siguiente. 

```{r}
# Prueba
cortest.bartlett(est_seg1)
```

Derivado de la anterior, ya que el valor p es casi 0 y éste es menor que 0.05 (0 < 0.05) se rechaza $H_0$, es decir, tiene sentido continuar con el análisis de factores. 

En conclusión, el análisis de factibilidad determina que es viable realizar el análisis de factores con la tabla de datos seleccionada.

# 8 Elección del número de factores.

## 8.1 Porcentaje de varianza explicada 

Para este criterio, por lo general se eligen el número de factores que explican de manera acumulada más del 70% de la varianza total. Para ello se obtienen los valores propios de la matriz de correlaciones y se divide entre el total de las variables de estudio. Posteriormente, se efectúa la suma acumulada para determinar el porcentaje de varianza explicada, según el número de factores seleccionados. 

```{r}
# Evitar la notación científica
options(scipen = 999)
# Se extraen los valores propios de la matriz de correlaciones
val_propios<-eigen(cor_seg1)$values
# Porcentaje de varianza explicada
acumulado<-cumsum(val_propios*100/ncol(est_seg1))
round(acumulado, 2)
# Gráfico de la varianza acumulada
barplot(acumulado, col = "lightblue", border = "white", names.arg = 1:12, 
        main = "Porcentaje de varianza explicada", 
        xlab = "Número de factores", ylab = "Porcentaje de varianza")
abline(h = 70, lty = 2, col = "red", lwd = 2)
box()
``` 

De acuerdo con las salidas previas, con tres factores se supera el 70% del total de la varianza explicada (línea roja punteada del gráfico), ya que se obtiene el 76.41%, con cuatro factores se llega al 83.39%, y así sucesivamente.


## 8.2 Criterio de Kaiser

Es uno de los criterios más utilizados por los paquetes estadísticos para determinar el número adecuado de factores. Se aplica cuando se trabaja con la matriz de correlaciones muestrales *R*. Consiste en elegir el número de valores propios de *R* que sean mayores a 1. Esta regla busca que cualquier factor retenido contenga al menos la varianza de una de las variables utilizadas en el análisis. 

```{r}
# Se imprimen los valores propios a dos decimales
round(val_propios, 2)
```
 
Con base a este criterio y a la salida anterior, se deberían elegir 3 factores, ya que sus valores propios son de 5.09, 2.52, 1.57 respectivamente.

## 8.3 Gráfico de sedimentación

Este criterio complementa al anterior y se basa también en el análisis de la magnitud de los valores propios, pero a partir de la tendencia que se observa en el gráfico. Se procura seleccionar un grupo reducido de factores que tengan valores propios significativamente superiores a los demás, para lo cual se identifica el punto de inflexión en la curva del gráfico de sedimentación (también referido como el codo por su semejanza con un brazo) a partir del cual la curva se transforma a una línea “plana” o relativamente recta. El gráfico para este ejercicio se presenta a continuación.


```{r}
# Gráfico de sedimentación
scree(est_seg1, main = "Gráfico de sedimentación", pc=FALSE, fa=TRUE)
```

En la gráfica previa se aprecia un punto de inflexión después de 2 factores para AF.

## 8.4 Análisis paralelo

El análisis paralelo suele complementar los anteriores cuando el número de variables iniciales y factores resultantes es elevado. El procedimiento se basa en el principio de que los factores a extraer deben dar cuenta de más varianza que la esperada de manera aleatoria. El procedimiento reordena las observaciones de manera aleatoria entre cada variable y los valores propios son recalculados a partir de esta nueva base de datos aleatoriamente ordenada. Los factores con valores propios mayores a los valores aleatorios son retenidos para interpretación.


Se comparan gráficamente las dos líneas del gráfico de sedimentación, una para el análisis de los datos reales y otra para el análisis de los datos aleatorios. El número de factores se determina con la intersección de ambas gráficas. El resultado de este criterio se muestra en la próxima gráfica.  

```{r,warning=FALSE}
# Análisis paralelo
fa.parallel(est_seg1, main = "Análisis paralelo")
```
De acuerdo con el análisis paralelo, sugiere conservar 3 factores en el caso de un AF y 2 componentes para un ACP. Hasta el momento los análisis sugieren considerar 3 factores.
Hasta el momento, las pruebas sugieren 3 factores.


# 9. Análisis factorial confirmatorio  

Para el análisis factorial confirmatorio se obtiene la solución por máxima verosimilitud de **L** (cargas) y $\mathbf{\Psi}$ (varianzas específicas) para 3 factores, considerando una rotación *varimax* para facilitar la interpretación de los factores.

## 9.1 Porcentaje de varianza explicada 

La rotación de las cargas busca mejorar la interpretación de los factores. Al rotarlos, la suma de los valores propios no se altera, pero podrían cambiar los valores propios, el porcentaje de varianza explicada y los valores de carga de cada factor. Por lo anterior, el porcentaje de varianza explicada por cada uno de los modelos propuestos se muestra a continuación.

```{r}
# Cargas estimadas rotadas para 3 factores por máxima verosimilitud
af.mv3<-factanal(est_seg1, factors = 3, rotation = "varimax", n.obs = nrow(est_seg1))
# Se almacenan las cargas
cargas_mv3<-af.mv3$loadings
vx3<-colSums(cargas_mv3^2)
# Se imprime la proporción de varianza explicada
rbind(`SS loadings` = vx3, `Proporción de la varianza` = vx3/nrow(cargas_mv3),
      `Varianza acumulada` = cumsum(vx3/nrow(cargas_mv3)))
```

De acuerdo al resultado previo, si se consideran 3 factores el porcentaje de la varianza acumulada es del 69.02%.

## 9.2 Estimación de cargas mediante máxima verosimilitud

Se realizaron las estimaciones de las cargas rotadas, las **comunalidades**, que es la parte de la variación de la variable que es compartida con otras variables, y las **varianzas únicas o específicas**, que es la parte de la variación de la variable que es propia de esa variable, para el modelo de 3 factores.

```{r}
# Modelo con 3 factores
# Comunalidades
comu_mv3<-diag(cargas_mv3[, 1:3] %*% t(cargas_mv3[, 1:3]))
# Unicidades o varianza específica
espe_mv3<-1 - comu_mv3

# Se unen los resultados para el modelo de 5 factores
res_mv3<-data.frame(CARGA3 = cargas_mv3[, 1:3], COM3 = comu_mv3, ESP3 = espe_mv3)
# Se imprime el resultado de las cargas rotadas
kbl(res_mv3, longtable = T, booktabs = T, digits = 3,
    caption = "Modelo con  factores por máxima verosimilitud", 
    col.names = c("F_1", "F_2", "F_3", "Comunalidad", "Varianza única"))
```

En la salida anterior se puede apreciar que, la varianza única de *seg_17_2023* (Variación porcentual del índice de conflictividad en la entidad federativa) es la más alta con 0.785, lo que implica que los factores no están explicando de buena forma la variabilidad de esta variable, asimismo, *seg_13_2023* (Tasa de policías por cada cien mil habitantes), *seg_9_2023* (Promedio de costos del delito en unidades económicas, por entidad federativa) cuentan con varianzas específicas de 0.652 y 0.603 de forma respectiva.  

## 9.3 Prueba de hipótesis para el número de factores comunes  

El supuesto de normalidad permite realizar pruebas sobre la suficiencia del modelo. Es decir, se puede probar si el número de factores *m* es suficiente para explicar la covarianza observada. Suponiendo que el modelo de *m* factores comunes tiene buen ajuste, entonces $\mathbf{\Sigma} = \mathbf{LL}'$ + $\mathbf{\Psi}$, y probar el ajuste del modelo de *m* factores comunes es equivalente a probar:
$H_0$: $\mathbf{\Sigma} = \mathbf{LL}'$ + $\mathbf{\Psi}$ (El número de factores en el modelo, es suficiente para capturar la dimensionalidad completa del conjunto de datos).
\newline $H_1$: $\mathbf{\Sigma} \ne \mathbf{LL}'$ + $\mathbf{\Psi}$ (El número de factores en el modelo, no es suficiente para capturar la dimensionalidad completa del conjunto de datos).  

La regla de decisión es rechazar $H_0$ si el valor p es menor que 0.05. Tal resultado indica que el número de factores es demasiado pequeño. Por el contrario, no rechazamos $H_0$ si el valor p excede 0.05. Tal resultado indica que es probable que haya suficientes (o mas que suficientes) factores que capturen la dimensionalidad completa del conjunto de datos. 


El resultado de la prueba para el modelo de 3 factores es el siguiente.

 Prueba de hipótesis

```{r}
# Valor p para el modelo de 3 factores
af.mv3$PVAL
```

Como el valor p es menor a 0.05 (0.00263155<0.05) se rechaza $H_0$, es decir, la prueba sugiere que 3 factores en el modelo no son suficientes para capturar la dimensionalidad completa del conjunto de datos.

**Nota**: Se realizó la prueba con distintos números de factores (5 y 6) y resultó que la con ese número de factores se acepta la prueba. Ver en el anexo
Por el momento, se trabajará con 3 factores, aunque la prueba de hipótesis no se haya aprobado, sin embargo, las demás pruebas para elegir el número de factores sugieren 3 factores.

## 9.4 Matriz de residuales

Se realiza la comparación de las matrices de correlación, en el cual se obtiene la diferencia entre la matriz de correlaciones real y la matriz reproducida mediante *m* factores, si el número de factores *m* es adecuado, la diferencia debe ser cercana a 0. Para ello se calculan las correlaciones reproducidas, que es la matriz de correlaciones de las variables originales que resultaría suponiendo que es correcto el número de factores retenidos. Asimismo, se calcula la matriz de correlaciones residuales, que es la matriz de las diferencias entre la matriz de correlaciones reproducida y la matriz de correlaciones reales. Si el número de factores *m* es adecuado, se esperaría que la matriz de correlaciones residuales tenga todas sus entradas cercanas a 0. 

El resultado para el modelo de 3 factores es el siguiente.

```{r}
# Matriz reproducida (3 factores)
mat_rep_mv3<-(cargas_mv3[, 1:3] %*% t(cargas_mv3[, 1:3])) + diag(espe_mv3)
# Matriz de residuales (3 factores)
mat_res_mv3<-cor_seg1 - mat_rep_mv3
# Gráficos de correlaciones
corrplot(mat_res_mv3, order = "original", tl.col = "black", tl.cex = 0.5, 
         mar = c(0, 0, 2, 0), title = "Matriz de residuales para 3 factores por MV")
```

Con base a la gráfica previa, se aprecia que la mayoría de las celdas son cercanas a 0, ya que no se observan círculos de color intenso. Los residuales más grandes que pueden distinguirse son para los pares *(seg_13_2023, seg_17_2023)*, *(seg_11_2023, seg_12_2023)* y *(seg_11_2023, seg_17_2023)* cuyos valores son: 0.2723, 0.2089 y -0.1910, de forma respectiva. En la gráfica se observa que el residual mayor tiene un valor de 0.2723671, y de manera general se observan algunos pares de variables con valores mayores a cero.

Hasta el momento, con el modelo de 3 factores se explica el 69% de la varianza, las demás pruebas para elegir el número de factores también sugieren tener 3 factores, además, la matriz de residuales arrojó valores cercanos de cero, sin embargo, la prueba de hipótesis para el número de factores comunes determinó que 3 factores no son suficientes, peso a esto, se seguirán considerando 3 factores por dos razones, la primera es por seguir el principio de parsimonía estadística y la segunda para comparar ese mismo número de factores respecto a componentes principales.



## 9.5 Interpretación de los factores  

La interpretación de las cargas de los modelos, generalmente se explican en términos de las cargas principales en cada factor. Para ello se realizó el siguiente gráfico para el modelo de 3 factores.  


```{r}
# Gráfico para interpretaciones de los factores
fa.diagram(af.mv3$loadings, main = "Modelo de 3 factores", cex = 1, rsize = 0.7)
```

1.- Para el diagrama con 3 factores, el **factor 1** se asocia con las variables:*seg_16_2023*,*seg_15_2023*, *seg_14_2023* y *seg_13_2023*, éstas variables son: Desconfianza en ministerios públicos, Desconfianza en jueces, Desconfianza en la policía estatal y municipal y Tasa de policías por cada cien mil habitantes.

2.- El **factor 2** está asociado con las variables: *seg_7_2023*, *seg_1_2023*, *seg_11_2023* y *seg_12_2023*, estas variables son: Índice de Riesgo por Crimen Organizado , Tasa de homicidios por cada cien mil habitantes, Porcentaje de personas que consideran insegura su entidad federativa y Porcentaje de personas que cambió algún hábito por temor a sufrir algún delito.

3.- El **factor 3** está asociado con las variables: *seg_2_2023*, *seg_3_2023*, *seg_9_2023* y *seg_17_2023*, estas variables son: Incidencia delictiva por cada cien mil habitantes, Prevalencia delictiva por cada cien mil habitantes, Promedio de costos del delito en unidades económicas, por entidad federativa y Variación porcentual del índice de conflictividad en la entidad federativa.

## 9.6 Estimación de factor scores 

Para la estimación de los vectores de puntajes factoriales (factor scores), se implementaron los métodos más consistentes y utilizados, los cuales son el de mínimos cuadrados ponderados (Bartlett) y el método por regresión (Thompson). El método por mínimos cuadrados ponderados supone que el vector de valores de los factores para cada observación es un parámetro a estimar. El método por regresión supone que estos vectores son variables aleatorias. El resultado de la estimación de los factor scores para el modelo de 3 factores se presenta a continuación.

Cálculo de factor scores mediante enfoque de mínimos cuadrados


```{r}
# Cálculo de factor scores mediante enfoque de mínimos cuadrados
scores_min_v2<-factanal(est_seg1, factors = 3, scores = "Bartlett")$scores
# Identificadores de los registros
ENT<-formatC(datos$cve, width = 2, format = "d", flag = "0")
# Se crea el data frame con los resultados
res_score1_v2<-data.frame(ID = paste0(ENT), scores_min_v2)
# Se ordenan los resultados por ID
res_score1_v2<-res_score1_v2[order(res_score1_v2$ID), ]

# Se imprime el resultado por mínimos cuadrados
kbl(res_score1_v2[1:32, ], longtable = T, booktabs = T, escape = FALSE, digits = 3,
    col.names = c("Entidad", "Score 1", "Score 2", "Score 3"), row.names = FALSE,
    caption = "Factor Scores estimados por Mínimos cuadrados") %>%
add_header_above(c(" ", "Mínimos cuadrados" = 3)) %>% 
kable_styling(latex_options = c("repeat_header"))   
```
Cálculo de factor scores mediante enfoque de regresión

```{r}
# Cálculo de factor scores mediante enfoque de regresión
scores_reg_v2<-factanal(est_seg1, factors = 3, scores = "regression")$scores
# Identificadores de los registros
ENT<-formatC(datos$cve, width = 2, format = "d", flag = "0")
# Se crea el data frame con los resultados
res_score2_v2<-data.frame(ID = paste0(ENT), scores_reg_v2)
# Se ordenan los resultados por ID
res_score2_v2<-res_score2_v2[order(res_score2_v2$ID), ]

# Se imprime el resultado por regresión
kbl(res_score2_v2[1:32, ], longtable = T, booktabs = T, escape = FALSE, digits = 3,
    col.names = c("Entidad", "Score 1", "Score 2", "Score 3"), row.names = FALSE,
    caption = "Factor Scores estimados por regresión") %>%
add_header_above(c(" ", "Regresión" = 3)) %>% 
kable_styling(latex_options = c("repeat_header"))   
```

Cálculo de las diferencias

```{r}
# Cálculo de las diferencias
diferencia_v2<-scores_min_v2 - scores_reg_v2
# Identificadores de los registros
ENT<-formatC(datos$cve, width = 2, format = "d", flag = "0")
# Se crea el data frame con los resultados
res_score3_v2<-data.frame(ID = paste0(ENT), diferencia_v2)
# Se ordenan los resultados por ID
res_score3_v2<-res_score3_v2[order(res_score3_v2$ID), ]

# Se imprime el resultado de las diferencias
kbl(res_score3_v2[1:32, ], longtable = T, booktabs = T, escape = FALSE, digits = 3,
    col.names = c("Entidad", "Score 1", "Score 2", "Score 3"), row.names = FALSE,
    caption = "Diferencias") %>%
add_header_above(c(" ", "Diferencias" = 3)) %>% 
kable_styling(latex_options = c("repeat_header"))  
# Cálculo del promedio de las diferencias
round(apply(diferencia_v2, 2, mean), 3)
# Cálculo de la desviación estándar de las diferencias
round(apply(diferencia_v2, 2, sd), 3)
```
La tablas anteriores contienen las estimaciones de los factor scores para las 32 entidades. Asimismo, se muestra el promedio de las diferencias (prácticamente 0) y sus desviaciones estándar (cercanas a 0) entre los enfoques de mínimos cuadrados y regresión. Se puede apreciar que las diferencias son mínimas, así que las interpretaciones que se den sobre las entidades serán aplicables a ambos enfoques. Por lo anterior, se determinó trabajar con los factor scores estimados mediante regresión.


## 9.7 Interpretación de resultados

Se considera distinguir a las 32 entidades federativas. Para facilitar la interpretación de los resultados, se realizaron las diferentes parejas posibles de los factor scores, además de dividir la gráfica en 4 cuadrantes. Asimismo, se identificaron los puntos extremos de cada cuadrante (representados con color rojo) para analizar su información. El resultado se muestra a continuación.


Se realiza el gráfico del Factor 1 vs Factor 2

```{r}

plot(res_score2_v2[, 2], res_score2_v2[, 3],main="Factor 1 vs Factor 2",xlim=c(-3.5,3.5),ylim=c(-3.5,3.5),
 xlab="Factor 1",ylab="Factor 2",type="n",lwd=2)
text(res_score2_v2[, 2],res_score2_v2[, 3],
 labels=abbreviate(row.names(res_score2_v2),minlength=8),cex=0.6,lwd=2, col = "blue")
abline(v = 0, h = 0, lty =2, col ="red")

```

Como se observa en el gráfico, la entidad 9 (CDMX), registra el mayor valor en le factor 1, relacionado a la desconfianza en instituciones y la tasa de policías, en cambio, el factor 2 la CDMX reporta valores por debajo de la media nacional y se puede entender porque el factor 2 explica la percepción de inseguridad, índice de riesgo y la tasa de homicidios. La entidad 15 (EDOMEX), también registra un alto valor en el factor 1, pero en el factor 2 el valor registrado es parecido a la media nacional.
Por otro lado, la entidad 19 (Nuevo León), reporta un valor bajó en el factor 1 y un valor parecido al de la media nacional en el factor 2.
Por su parte, la entidad 32 (Zacatecas), registra un valor por debajo de la media nacional en el factor 1, sin embargo, es el que registra el valor más alto respecto al factor 2.
Finalmente, otra entidad que está en un punto extremo es la entidad 31 (Yucatán), en donde está dentro del top de valores mínimos del factor 1 y también se registra el menor valor del factor 2.   

Se realiza el gráfico del Factor 1 vs Factor 3

```{r}
plot(res_score2_v2[, 2], res_score2_v2[, 4],main="Factor 1 vs Factor 3",xlim=c(-3.5,3.5),ylim=c(-3.5,3.5),
 xlab="Factor 1",ylab="Factor 3",type="n",lwd=2)
text(res_score2_v2[, 2],res_score2_v2[, 4],
 labels=abbreviate(row.names(res_score2_v2),minlength=8),cex=0.6,lwd=2, col = "blue")
abline(v = 0, h = 0, lty =2, col ="red")
```
En la gráfica anterior, se muestra como la entidad 9 (CDMX), registra valores máximos para el factor 1 y también está dentro del top de los valores máximos para el factor 3, este factor hace referencia a la incidencia delictiva, la prevalencia delictiva, el promedio del costo del delito y la variación porcentual del índice de conflictividad. Un comportamiento similar tiene la entidad 15 (EDOMEX).
En otro cuadrante se encuentra la entidad 19 (Nuevo León), en donde se registran los valores mínimos del factor 1 en contraste con el registro de valores máximos para el factor 3.
Por otro lado, la entidad 31 (YUcatán) reporta un valor por debajo del nivel nacional para el factor 1 y para el factor 3 reporta un valor muy parecido al del nacional para el factor 3.
En otro extremo del otro cuadrante, se encuentra la entidad 16 (Michoacán de Ocampo), en donde se registran valores por encima de la media nacional para el factor 1 en contraste con el factor 3 que registra uno de los valores mínimos, es decir, hay cierto nivel de desconfianza en las autoridades y bajos índices delictivos , costos por promedio y variación del índide de conflictividad.


Se realiza el gráfico del Factor 2 vs Factor 3

```{r}

plot(res_score2_v2[, 3], res_score2_v2[, 4],main="Factor 2 vs Factor 3",xlim=c(-3.5,3.5),ylim=c(-3.5,3.5),
 xlab="Factor 2",ylab="Factor 3",type="n",lwd=2)
text(res_score2_v2[, 3],res_score2_v2[, 4],
 labels=abbreviate(row.names(res_score2_v2),minlength=8),cex=0.6,lwd=2, col = "blue")
abline(v = 0, h = 0, lty =2, col ="red")
```

En la gráfica anterior, se muestra que la entidad 2 (Baja California) reporta valores por encima de la media nacional para el factor 2 y valores cercanos a la media nacional para el factor 3. 
En otro cuadrante está la entidad 19 (Nuevo León) en donde se reporta un valor cercano a media nacional para el factor 2 y el máximo valor para el factor 3. Por otro lado, está la entidad 31 (Yucatán) en donde se registra el valor mínimo en el factor 2 y un valor cercano a la media nacional para el factor 3.
En otro cuadrante, la entidad 10 (Durango), registra un valor mínimo para el factor 2 y también para el factor 3, pues ambos están por debajo de la media nacional.
En otro cuadrante está la entidad 16, en donde registra un valor por encima de la media nacional para el factor 2 y un valor mínimo para el factor 3. Finalmente, en otro extremo se encuentra la entidad 32 (Zacatecas), que reporta un valor alto en el factor 2  y un valor por debajo de la media nacional para el factor 3


## 9.8 Cálculo del subíndice de seguridad

Opción 1. Como se observa a continuación, se utiliza forma de cálculo que se describe en la *NOTA 1*, solo que en este caso, se obtienen los  factor scores por el método de análisis de factores considerando 3 factores.


```{r}
#Se extraen los 3 factor scores
bd_score6<-res_score2_v2
#Se convierte bd_factor_score en data frame 
bd_score6<- as.data.frame(bd_score6)
bd_score6$media_seg<- apply(bd_score6[,c(2:4)],1,mean)
val_min<- min(bd_score6$media_seg)# -0.8749612
bd_score6$dif<-(bd_score6$media_seg-val_min)
val_max<- max(bd_score6$dif) #2.154723
bd_score6$ind_seg<-(bd_score6$dif/val_max)*100
#bd_score6 #La variable entidad ya está incluída
```
Opción 2. Como se observa a continuación, se utiliza forma de cálculo que se describe en la *NOTA 2*, solo que en este caso, se obtienen los  factor scores por el método de análisis de factores considerando 3 factores.

```{r}
#Se extraen los 3 factor scores
bd_score7<-res_score2_v2
#Se convierte en data frame
bd_score7<- as.data.frame(bd_score7)
val_min1<- min(bd_score7$Factor1)
val_min2<- min(bd_score7$Factor2)
val_min3<- min(bd_score7$Factor3)
bd_score7$dif1<-(bd_score7$Factor1-val_min1)
bd_score7$dif2<-(bd_score7$Factor2-val_min2)
bd_score7$dif3<-(bd_score7$Factor3-val_min3)
val_max1<- max(bd_score7$dif1)
val_max2<- max(bd_score7$dif2)
val_max3<- max(bd_score7$dif3)
bd_score7$ind_seg1<-(bd_score7$dif1/val_max1)*100
bd_score7$ind_seg2<-(bd_score7$dif2/val_max2)*100
bd_score7$ind_seg3<-(bd_score7$dif3/val_max3)*100
bd_score7$ind_seg_f<-apply(bd_score7[,c(8:10)],1,mean)
```

Con base en lo anterior, se hace una tabla resumen de los índices calculados.


```{r,warning=FALSE}
tr<-merge(bd_score, bd_score1, by = c("ID"))
tr<-merge(tr, bd_score2, by = c("ID"))
tr<-merge(tr, bd_score3, by = c("ID"))
tr<-merge(tr, bd_score4, by = c("ID"))
tr<-merge(tr, bd_score5, by = c("ID"))
tr<-merge(tr, bd_score6, by = c("ID"))
tr<-merge(tr, bd_score7, by = c("ID"))
tabla<-tr[,c(1,9,25,33,49,55,65,71,81)]
tabla$ent <- c('AGS', 'BC', 
          'BCS', 'CAMP', 'COAH', 'COL', 
          'CHIS', 'CHIH', 'CDMX', 'DGO', 'GTO',
          'GRO', 'HGO', 'JAL', 'MEX', 
          'MICH', 'MOR', 'NAY', 'NL', 'OAX', 
          'PUE', 'QRO', 'QR', 'SLP', 'SIN', 
          'SON', 'TAB', 'TAMPS', 'TLAX', 
          'VER', 'YUC', 'ZAC')

#Cambiando nombre
names (tabla) = c("CVE_ENT", "cp5_opc1", "cp5_opc2", "afc5_opc1", 
                  "afc5_opc2", "cp3_opc1", "cp3_opc2","afc3_opc1",
                  "afc3_opc2", "ent")
tabla$CVE_ENT <- as.numeric(tabla$CVE_ENT)
tabla_final<- tabla[,c(10,2,3,4,5,6,7,8,9)]

```

\newpage

```{r}
# Formato de tabla
kbl(tabla_final, longtable = T, booktabs = T, digits = 3,
    col.names = c("ent", "cp5_opc1", "cp5_opc2", "afc5_opc1", 
                  "afc5_opc2", "cp3_opc1", "cp3_opc2","afc3_opc1",
                  "afc3_opc2"), row.names = FALSE,
    caption = "Resultados del subíndice de riesgo de seguridad") %>%
kable_styling(latex_options = c("repeat_header"))   
```

\newpage
 
Como se observa en la tabla resumen, se están representando las 8 opciones de subíndices que se construyeron, dado que se está midiendo un riesgo, bajo el supuesto de que el riesgo siempre existe, entonces, éste no puede tener valor de cero, en ese sentido, los subíndices "cp5_opc1", "afc5_opc1", "cp3_opc1" y "afc3_opc1" no se considerarán como opción factible, por lo tanto no se considerarían.


# 10 Mapas de calor por entidad federativa

Se realizan mapas de calor por cada una de las 8 opciones que se realizaron de los subindices de seguridad, con la finalidad de comparar los resultados.
Se lee la capa en el formato shp

```{r, warning=FALSE}
# Leer la capa de entidad federativa con la librería de sf
mexico <- st_read("C:/Users/User/Documents/ÍNDICE DE RIESGO/Bases de datos/Capas/00ent.shp")
# Convertir el objeto sf a un data frame para ggplot
mexico_df <- mexico %>%
  mutate(CVE_ENT = as.numeric(CVE_ENT)) %>%
  st_as_sf()
```

Se trabaja un mapa por subíndice calculado

```{r}
#Tabulado para el mapa 1 correspondiente al subíndice 1:cp5_opc1 
sub_cp5_opc1 <- tabla %>%
   dplyr::select(CVE_ENT, cp5_opc1) %>%
  mutate(cp5_opc1 = round(cp5_opc1, 1))  # Redondear a un decimal
# Unir los datos espaciales con los valores del subíndice
datos_mapa1 <- mexico_df %>%
  left_join(sub_cp5_opc1, by = "CVE_ENT")

#Tabulado para el mapa 2 correspondiente al subíndice 2:cp5_opc2 
sub_cp5_opc2 <- tabla %>%
  dplyr::select(CVE_ENT, cp5_opc2) %>%
  mutate(cp5_opc2 = round(cp5_opc2, 1))  # Redondear a un decimal
# Unir los datos espaciales con los valores del subíndice
datos_mapa2 <- mexico_df %>%
  left_join(sub_cp5_opc2, by = "CVE_ENT")

#Tabulado para el mapa 3 correspondiente al subíndice 3:afc5_opc1
sub_afc5_opc1 <- tabla %>%
dplyr::select(CVE_ENT, afc5_opc1) %>%
  mutate(afc5_opc1 = round(afc5_opc1, 1))  # Redondear a un decimal
# Unir los datos espaciales con los valores del subíndice
datos_mapa3 <- mexico_df %>%
  left_join(sub_afc5_opc1, by = "CVE_ENT")

#Tabulado para el mapa 4 correspondiente al subíndice 4:afc5_opc2
sub_afc5_opc2 <- tabla  %>%
  dplyr::select(CVE_ENT, afc5_opc2) %>%
  mutate(afc5_opc2 = round(afc5_opc2, 1))  # Redondear a un decimal
# Unir los datos espaciales con los valores del subíndice
datos_mapa4 <- mexico_df %>%
  left_join(sub_afc5_opc2, by = "CVE_ENT") 

#Tabulado para el mapa 5 correspondiente al subíndice 5:cp3_opc1
sub_cp3_opc1 <- tabla %>%
  dplyr::select(CVE_ENT, cp3_opc1) %>%
  mutate(cp3_opc1 = round(cp3_opc1, 1))  # Redondear a un decimal
# Unir los datos espaciales con los valores del subíndice
datos_mapa5 <- mexico_df %>%
  left_join(sub_cp3_opc1, by = "CVE_ENT") 

#Tabulado para el mapa 6 correspondiente al subíndice 6:cp3_opc2
sub_cp3_opc2 <- tabla %>%
    dplyr::select(CVE_ENT, cp3_opc2) %>%
  mutate(cp3_opc2 = round(cp3_opc2, 1))  # Redondear a un decimal
# Unir los datos espaciales con los valores del subíndice
datos_mapa6 <- mexico_df %>%
  left_join(sub_cp3_opc2, by = "CVE_ENT") 

#Tabulado para el mapa 7 correspondiente al subíndice 7:afc3_opc1
sub_afc3_opc1 <-  tabla %>%
    dplyr::select(CVE_ENT, afc3_opc1 ) %>%
  mutate(afc3_opc1  = round(afc3_opc1 , 1))  # Redondear a un decimal
# Unir los datos espaciales con los valores del subíndice
datos_mapa7 <- mexico_df %>%
  left_join(sub_afc3_opc1, by = "CVE_ENT") 

#Tabulado para el mapa 8correspondiente al subíndice 8:afc3_opc2
sub_afc3_opc2 <- tabla %>%
    dplyr::select(CVE_ENT, afc3_opc2 ) %>%
  mutate(afc3_opc2 = round(afc3_opc2 , 1))  # Redondear a un decimal
# Unir los datos espaciales con los valores del subíndice
datos_mapa8 <- mexico_df %>%
  left_join(sub_afc3_opc2, by = "CVE_ENT") 
```


Mapa del subíndice 1

```{r}
ggplot() +
  geom_sf(data = datos_mapa1, aes(fill = cp5_opc1), color = "gray") +
  scale_fill_gradient(low = "#FFE1AF", high = "#F76E11") +
  labs(
    title = "Subíndice 1",
    caption = "Elaboración propia.\nSubíndice realizado con 5 componentes principales, forma 1 de cálculo."
    ) +
  theme_minimal() +
  theme(plot.caption = element_text(size = 8))
```

Mapa del subíndice 2

```{r}
ggplot() +
  geom_sf(data = datos_mapa2, aes(fill = cp5_opc2), color = "gray") +
  scale_fill_gradient(low = "#FFE1AF", high = "#F76E11") +
  labs(
    title = "Subíndice 2",
    caption = "Elaboración propia.\nSubíndice realizado con 5 componentes principales, forma 2 de cálculo."
    ) +
  theme_minimal() +
  theme(plot.caption = element_text(size = 8))
```

Mapa del subíndice 3

```{r}
ggplot() +
  geom_sf(data = datos_mapa3, aes(fill = afc5_opc1), color = "gray") +
  scale_fill_gradient(low = "#FFE1AF", high = "#F76E11") +
  labs(
    title = "Subíndice 3",
    caption = "Elaboración propia.\nSubíndice realizado con 5 factores comunes, forma 1 de cálculo."
    ) +
  theme_minimal() +
  theme(plot.caption = element_text(size = 8))
```

Mapa del subíndice 4

```{r}
ggplot() +
  geom_sf(data = datos_mapa4, aes(fill = afc5_opc2), color = "gray") +
  scale_fill_gradient(low = "#FFE1AF", high = "#F76E11") +
  labs(
    title = "Subíndice 4",
    caption = "Elaboración propia.\nSubíndice realizado con 5 factores comunes, forma 2 de cálculo."
    ) +
  theme_minimal() +
  theme(plot.caption = element_text(size = 8))
```


Mapa del subíndice 5

```{r}
ggplot() +
  geom_sf(data = datos_mapa5, aes(fill = cp3_opc1), color = "gray") +
  scale_fill_gradient(low = "#FFE1AF", high = "#F76E11") +
  labs(
    title = "Subíndice 5",
    caption = "Elaboración propia.\nSubíndice realizado con 3 componentes principales, forma 1 de cálculo."
    ) +
  theme_minimal() +
  theme(plot.caption = element_text(size = 8))
```

Mapa del subíndice 6

```{r}
ggplot() +
  geom_sf(data = datos_mapa6, aes(fill = cp3_opc2), color = "gray") +
  scale_fill_gradient(low = "#FFE1AF", high = "#F76E11") +
  labs(
    title = "Subíndice 6",
    caption = "Elaboración propia.\nSubíndice realizado con 3 componentes principales, forma 2 de cálculo."
    ) +
  theme_minimal() +
  theme(plot.caption = element_text(size = 8))
```

Mapa del subíndice 7

```{r}
ggplot() +
  geom_sf(data = datos_mapa7, aes(fill = afc3_opc1), color = "gray") +
  scale_fill_gradient(low = "#FFE1AF", high = "#F76E11") +
  labs(
    title = "Subíndice 7",
    caption = "Elaboración propia.\nSubíndice realizado con 3 factores comunes, forma 1 de cálculo."
    ) +
  theme_minimal() +
  theme(plot.caption = element_text(size = 8))
```


Mapa del subíndice 8

```{r}
ggplot() +
  geom_sf(data = datos_mapa8, aes(fill = afc3_opc2), color = "gray") +
  scale_fill_gradient(low = "#FFE1AF", high = "#F76E11") +
  labs(
    title = "Subíndice 8",
    caption = "Elaboración propia.\nSubíndice realizado con 3 factores comunes, forma 2 de cálculo."
    ) +
  theme_minimal() +
  theme(plot.caption = element_text(size = 8))
```

# Conclusiones:

Como se describió en el documento, el análisis realizado estuvo enfocado en ver qué opción sería la más factible, para calcular el subíndice de riesgo, tomando como base los resultados obtenidos por entidad federativa, se sugiere que el subíndice 2: "cp5_opc2", realizado por componentes principales, es una buen opción, porque se están considerando a todas las variables, los resultados arrojados empatan con la realidad en términos de seguridad/inseguridad a nivel entidad federativa y finalmente, por cuestiones de parsimonia estadística, se sugiere considerar al subíndice 2, como la opción más factible.


\newpage

# Simbología:

1. seg: Es la base con todas las variables de interés y con la que se trabaja en la primera parte.
2. est_seg: Es la base de datos "seg" estandarizada
3. cor_seg: Es la matriz de correlación de las variables de la base "seg".
4. det_seg: Es el valor del determinante de la matriz de correlaciones "cor_seg"
5. seg.pc: Es el objeto en donde se realiza el análisis de componentes principales usando la base "est_seg", devolviendo los resultados como un objeto de clase princomp.

6. bd_score: Es la tabla que contiene los scores seleccionados (5) por componentes principales, el id (entidad federativa) además, contiene las variables auxiliares (media_seg y dif) para el cálculo del subíndice y finalmente, contiene la primera opción del subíndice de seguridad **ind_seg**. A continuación, se detalla cada una de las variables o valores usados en el cálculo.

 - 6.1.  media_seg: Media de los scores elegidos, por entidad federativa.
 - 6.2.  val_min: Es el valor mínimo de la variable *media_seg*.
 - 6.3.  dif: Es la diferencia de la media *media_seg* y el valor mínimo de la media *val_min*.
 - 6.4.  val_max: Es el valor máximo de la diferencia *dif*.
 - 6.5. ind_seg: Es el subíndice de seguridad, se obtiene al dividir la diferencia *dif* entre el valor máximo de la diferencia *val_max*, multiplicado por 100. 

7. bd_score1: Es la tabla que contiene los scores seleccionados (5) por componentes principales, el id (entidad federativa) además, contiene las variables auxiliares dif1, dif2, dif3, dif4, dif5, ind_seg1, ind_seg2, ind_seg3, ind_seg4 y ind_seg5 para el cálculo del subíndice y finalmente, contiene la segunda opción del subíndice de seguridad **ind_seg_f**. A continuación, se detalla cada una de las variables o valores usados en el cálculo.

 - 7.1. $val\_min_i:$ Es el valor mínimo de cada uno de los scores, con $i = 1,2,3...n$ y $n$ es el número de scores elegidos, en este ejemplo son 5.
 - 7.2. $dif_i:$ Es la diferencia del $score_i$ y el valor mínimo de cada score $val\_min_i$.
 - 7.3. $val\_max_i:$ Es el valor máximo para cada una de las diferencias $dif_i$.
 - 7.4. $ind\_seg_i:$ Es el subíndice "i" de cada score. Se obtiene al dividir la diferencia $dif_i$ entre el valor máximo de la diferencia $val\_max_i$, multiplicado por 100.
 - 7.5. ind_seg_f:Es el subíndice de seguridad (opción 2), se obtiene al promediar los $ind\_seg_i$.

8. af.mv5: Es el objeto en donde se realiza el análisis factorial de máxima verosimilitud con la base de datos estandarizada "est_seg". Se le puso el número 5 para referir que son 5 factores.
9. af.mv6: Es el objeto en donde se realiza el análisis factorial de máxima verosimilitud con la base de datos estandarizada "est_seg". Se le puso el número 6 para referir que son 6 factores.
10. comu_mv5: Comunalidades del modelo con 5 factores
11. espe_mv5: Varianza especifica del modelo con 5 factores
12. res_mv5: Es un data frame con los 5 factores, las comunalidades y la varianza única con un modelo de 5 factores.
13. comu_mv6: Comunalidades del modelo con 6 factores
14. espe_mv6: Varianza especifica del modelo con 6 factores
15. res_mv6: Es un data frame con los 6 factores, las comunalidades y la varianza única con un modelo de 6 factores.
16. af.mv5$PVAL: "Valor p" para el modelo de 5 factores con base en la prueba de hipótesis.
17. af.mv6$PVAL: "Valor p" para el modelo de 6 factores con base en la prueba de hipótesis.
18. mat_rep_mv5: Matriz reproducida con 5 factores, se puede interpretar como la matriz estimada a partir de los 5 factores.
19. mat_res_mv5: Matriz residual, es la matriz de diferencias entre la matriz de correlación (integrada con las variables originales) y la matriz repoducida (con la agruación de la variables en 5 factores propuestos)
20. mat_rep_mv6: Matriz reproducida con 6 factores, se puede interpretar como la matriz estimada a partir de los 6 factores.
21. mat_res_mv6: Matriz residual, es la matriz de diferencias entre la matriz de correlación (integrada con las variables originales) y la matriz repoducida (con la agrupación de las variables en 6 factores propuestos)
22. scores_min: Cálculo de factor scores mediante enfoque de mínimos cuadrados, estos son a nivel observación, en este caso son 32 por las entidades federativas.
23. scores_reg: Cálculo de factor scores mediante enfoque de regresión, estos son a nivel observación, en este caso son 32 por las entidades federativas.
24. diferencia: Es matriz en donde se hace la diferencia entre el cálculo de los factor scores con el método de mínimos cuadrados vs el de regresión. De esta manera si la diferencia es cercana a cero, las interpretaciones que se den sobre las entidades serán aplicables a ambos enfoques.
25. bd_score2: Es la tabla que contiene los factor scores seleccionados (5) por análisis de factores, el id (entidad federativa) además, contiene las variables auxiliares (media_seg y dif) para el cálculo del subíndice y finalmente, contiene la primera opción del subíndice de seguridad **ind_seg**. Las variables y valores usados para el cálculo del subíndice de seguridad son las mismas que las descritas en los puntos: 6.1 al 6.5
26. bd_score3: Es la tabla que contiene los factor scores seleccionados (5) por análisis de factores, el id (entidad federativa) además, contiene las variables auxiliares dif1, dif2, dif3, dif4, dif5, ind_seg1, ind_seg2, ind_seg3, ind_seg4 y ind_seg5 para el cálculo del subíndice y finalmente, contiene la segunda opción del subíndice de seguridad **ind_seg_f**. Las variables y valores usados para el cálculo del subíndice de seguridad son las mismas que las descritas en los puntos: 7.1 al 7.5
27. seg1: Es la base en donde se eliminan las variables que tienen poca correlación.
28. est_seg1: Es la base de datos "seg1" estandarizada.
29. cor_seg1: Es la matriz de correlación de las variables de la base "seg1".
30. det_seg1: Es el valor del determinante de la matriz de correlaciones "cor_seg1"
31. seg.pc1: Es el objeto en donde se realiza el análisis de componentes principales usando la base "est_seg1", devolviendo los resultados como un objeto de clase princomp.
32. bd_score4: Es la tabla que contiene los scores seleccionados (3) por componentes principales, el id (entidad federativa) además, contiene las variables auxiliares (media_seg y dif) para el cálculo del subíndice y finalmente, contiene la primera opción del subíndice de seguridad **ind_seg**. Las variables y valores usados para el cálculo del subíndice de seguridad son las mismas que las descritas en los puntos: 6.1 al 6.5
33. bd_score5: Es la tabla que contiene los scores seleccionados (3) por componentes principales, el id (entidad federativa) además, contiene las variables auxiliares dif1, dif2, dif3, ind_seg1, ind_seg2, ind_seg3, para el cálculo del subíndice y finalmente, contiene la segunda opción del subíndice de seguridad **ind_seg_f**. Las variables y valores usados para el cálculo del subíndice de seguridad son las mismas que las descritas en los puntos: 7.1 al 7.5.
34. af.mv3: Es el objeto en donde se realiza el análisis factorial de máxima verosimilitud con la base de datos estandarizada "est_seg1". Se le puso el número 3 para referir que son 3 factores.
35. comu_mv3: Comunalidades del modelo con 3 factores
36. espe_mv3: Varianza especifica del modelo con 3 factores
37. res_mv3: Es un data frame con los 3 factores, las comunalidades y la varianza única con un modelo de 3 factores.
38. af.mv3$PVAL: "Valor p" para el modelo de 3 factores con base en la prueba de hipótesis.
39. mat_rep_mv3: Matriz reproducida con 3 factores, se puede interpretar como la matriz estimada a partir de los 3 factores, usando "est_seg1"
40. mat_res_mv3: Matriz residual, es la matriz de diferencias entre la matriz de correlación (integrada con las variables originales),  usando "est_seg1" y la matriz repoducida (con la agrupación de las variables en 3 factores propuestos)
41. scores_min_v2: Cálculo de factor scores mediante enfoque de mínimos cuadrados, estos son a nivel observación, en este caso son 32 entidades federativas, considerando 3 factores y "est_seg1"
42. scores_reg_v2: Cálculo de factor scores mediante enfoque de regresión, estos son a nivel observación, en este caso son 32 entidades federativas, considerando 3 factores y "est_seg1"
43. diferencia_v2: Es matriz en donde se hace la diferencia entre el cálculo de los factor scores con el método de mínimos cuadrados vs el de regresión, considerando 3 factores y "est_seg1"
44. bd_score6: Es la tabla que contiene los factor scores seleccionados (3) por análisis de factores, el id (entidad federativa) además, contiene las variables auxiliares (media_seg y dif) para el cálculo del subíndice y finalmente, contiene la primera opción del subíndice de seguridad **ind_seg**. Las variables y valores usados para el cálculo del subíndice de seguridad son las mismas que las descritas en los puntos: 6.1 al 6.5
45. bd_score7: Es la tabla que contiene los factor scores seleccionados (3) por análisis de factores, el id (entidad federativa) además, contiene las variables auxiliares dif1, dif2, dif3, ind_seg1, ind_seg2 y ind_seg3 para el cálculo del subíndice y finalmente, contiene la segunda opción del subíndice de seguridad **ind_seg_f**. Las variables y valores usados para el cálculo del subíndice de seguridad son las mismas que las descritas en los puntos: 7.1 al 7.5.
46. tr: Base con todas las variables de las tablas bd_score al bd_score7, la unión se realizó por el ID de la entidad federativa, la base incluye los 8 subíndices.
47. tabla: Es la base con los 8 subíndices cálculados por la entidad federativa. Se realizó un cambio del nombre de los subíndices asociando el método usado, el número de componentes o factores elegidos y la opción de cálculo del subíndice. Además, se incluye la variable "ent", que refiere a los nombres de las entidades federativas. A continuación, se detalla:

 - 47.1. cp5_opc1: Opción 1 del subíndice, se extrajo de "bd_score".
 - 47.2. cp5_opc2: Opción 2 del subíndice, se extrajo de "bd_score1".
 - 47.3. afc5_opc1: Opción 3 del subíndice, se extrajo de "bd_score2".
 - 47.4. afc5_opc2: Opción 4 del subíndice, se extrajo de "bd_score3".
 - 47.5. cp3_opc1: Opción 5 del subíndice, se extrajo de "bd_score4".
 - 47.6. cp3_opc2: Opción 6 del subíndice, se extrajo de "bd_score5".
 - 47.7. afc3_opc1: Opción 7 del subíndice, se extrajo de "bd_score6".
 - 47.8. afc3_opc2: Opción 8 del subíndice, se extrajo de "bd_score7".
   
48. tabla_final: Es la tabla con el formato final, integrada por la variable "ent" y los 8 subíndices descritos en el punto anterior.

\newpage

# ANEXO

3 factores y 4 factores.

Para 3 factores

```{r}
# Cargas estimadas rotadas para 3 factores por máxima verosimilitud
af.mv3<-factanal(est_seg, factors = 3, rotation = "varimax", n.obs = nrow(est_seg))
# Se almacenan las cargas
cargas_mv3<-af.mv3$loadings
vx3<-colSums(cargas_mv3^2)
# Se imprime la proporción de varianza explicada
rbind(`SS loadings` = vx3, `Proporción de la varianza` = vx3/nrow(cargas_mv3),
      `Varianza acumulada` = cumsum(vx3/nrow(cargas_mv3)))
```
 Prueba de hipótesis

```{r}
# Valor p para el modelo de 3 factores
af.mv3$PVAL
```
Como el valor p es menor a 0.05 (0.01429138  < 0.05) se rechaza $H_o$, es decir, 3 factores en el modelo no son suficientes para capturar la dimensionalidad completa del conjunto de datos.

Para 4 factores

```{r}
# Cargas estimadas rotadas para 5 factores por máxima verosimilitud
af.mv4<-factanal(est_seg, factors = 4, rotation = "varimax", n.obs = nrow(est_seg))
# Se almacenan las cargas
cargas_mv4<-af.mv4$loadings
vx4<-colSums(cargas_mv4^2)
# Se imprime la proporción de varianza explicada
rbind(`SS loadings` = vx4, `Proporción de la varianza` = vx4/nrow(cargas_mv4),
      `Varianza acumulada` = cumsum(vx4/nrow(cargas_mv4)))
```

```{r}
# Valor p para el modelo de 2 factores
af.mv4$PVAL
```

Como el valor p es mayor a 0.05 (0.06523034 > 0.05) no se rechaza $H_0$, es decir, 4 factores en el modelo son suficientes para capturar la dimensionalidad completa del conjunto de datos.
Aunque, no se rechaza $H_0$, en valor de p es muy cercano a 0.05.

VERSIÓN 2

Para 4 factores

```{r}
# Cargas estimadas rotadas para 4 factores por máxima verosimilitud
af.mv4_v2<-factanal(est_seg1, factors = 4, rotation = "varimax", n.obs = nrow(est_seg1))
# Se almacenan las cargas
cargas_mv4_v2<-af.mv4_v2$loadings
vx4_v2<-colSums(cargas_mv4_v2^2)
# Se imprime la proporción de varianza explicada
rbind(`SS loadings` = vx4_v2, `Proporción de la varianza` = vx4_v2/nrow(cargas_mv4_v2),
      `Varianza acumulada` = cumsum(vx4_v2/nrow(cargas_mv4_v2)))
```
 Prueba de hipótesis

```{r}
# Valor p para el modelo de 4 factores
af.mv4_v2$PVAL
```
Como el valor p es menor a 0.05 (0.01137698   < 0.05) se rechaza $H_0$, es decir, 4 factores en el modelo no son suficientes para capturar la dimensionalidad completa del conjunto de datos.


Para 5 factores

```{r}
# Cargas estimadas rotadas para 5 factores por máxima verosimilitud
af.mv5_v2<-factanal(est_seg1, factors = 5, rotation = "varimax", n.obs = nrow(est_seg1))
# Se almacenan las cargas
cargas_mv5_v2<-af.mv5_v2$loadings
vx5_v2<-colSums(cargas_mv5_v2^2)
# Se imprime la proporción de varianza explicada
rbind(`SS loadings` = vx5_v2, `Proporción de la varianza` = vx5_v2/nrow(cargas_mv5_v2),
      `Varianza acumulada` = cumsum(vx5_v2/nrow(cargas_mv5_v2)))
```
 Prueba de hipótesis

```{r}
# Valor p para el modelo de 5 factores
af.mv5_v2$PVAL
```
Como el valor p es mayor a 0.05 (0.06135688 > 0.05) no se rechaza $H_0$, es decir, 5 factores en el modelo son suficientes para capturar la dimensionalidad completa del conjunto de datos.

Para 6 factores

```{r}
# Cargas estimadas rotadas para 6 factores por máxima verosimilitud
af.mv6_v2<-factanal(est_seg1, factors = 6, rotation = "varimax", n.obs = nrow(est_seg1))
# Se almacenan las cargas
cargas_mv6_v2<-af.mv6_v2$loadings
vx6_v2<-colSums(cargas_mv6_v2^2)
# Se imprime la proporción de varianza explicada
rbind(`SS loadings` = vx6_v2, `Proporción de la varianza` = vx6_v2/nrow(cargas_mv6_v2),
      `Varianza acumulada` = cumsum(vx6_v2/nrow(cargas_mv6_v2)))
```
 Prueba de hipótesis

```{r}
# Valor p para el modelo de 6 factores
af.mv6_v2$PVAL
```
Como el valor p es mayor a 0.05 (0.1153948  > 0.05) no se rechaza $H_0$, es decir, 6 factores en el modelo son suficientes para capturar la dimensionalidad completa del conjunto de datos.



